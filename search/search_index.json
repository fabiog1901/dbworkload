{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dbworkload - DBMS workload utility","text":""},{"location":"#overview","title":"Overview","text":"<p>The goal of <code>dbworkload</code> is to ease the creation and execution of bespoke database workload scripts.</p> <p>The user is responsible for coding the workload logic as a Python <code>class</code>, while <code>dbworkload</code> takes care of providing ancillary features, such as configuring the workload concurrency, duration and/or iteration, and more.</p> <p>The user, by coding the class, has complete control of the workload flow: what transactions are executed in which order, and what statements are included in each transaction, plus what data to insert and how to generate it and manipulate it.</p>"},{"location":"#database-seeding","title":"Database seeding","text":"<p><code>dbworkload</code> can help with seeding a database by creating CSV files with random generated data, whose definition is supplied in a YAML file and can be extracted from a DDL SQL file.</p>"},{"location":"#how-it-works","title":"How it works","text":"<p>It\u2019s helpful to understand first what <code>dbworkload</code> does:</p> <ul> <li> <p>At runtime, <code>dbworkload</code> first imports the class you pass, <code>bank.py</code>.</p> </li> <li> <p>It spawns n threads for concurrent execution (see next section on Concurrency).</p> </li> <li>By default, it sets the connection to <code>autocommit</code> mode.</li> <li>Each thread creates a database connection - no need for a connection pool.</li> <li>In a loop, each <code>dbworkload</code> thread will:</li> <li>execute function <code>loop()</code> which returns a list of functions.</li> <li>execute each function in the list sequentially. Each function, typically, executes a SQL statement/transaction.</li> <li>Execution stats are funneled back to the MainThread, which aggregates and, optionally, prints them to stdout and saves them to a CSV file.</li> <li>If the connection drops, it will recreate it. You can also program how long you want the connection to last.</li> <li><code>dbworkload</code> stops once a limit has been reached (iteration/duration), or you Ctrl+C.</li> </ul>"},{"location":"#concurrency-processes-and-threads","title":"Concurrency - processes and threads","text":"<p><code>dbworkload</code> uses both the <code>multiprocessing</code> and <code>threading</code> library to achieve high concurrency, that is, opening multiple connections to the DBMS.</p> <p>There are 2 parameters that can be used to configure how many processes you want to create, and for each process, how many threads:</p> <ul> <li><code>--procs/-x</code>, to configure the count of processes (defaults to the CPU count)</li> <li><code>--concurrency/-c</code>, to configure the total number of connections (also referred to as executing threads)</li> </ul> <p><code>dbworkload</code> will spread the load across the processes, so that each process has an even amount of threads.</p> <p>Example: if we set <code>--procs 4</code> and <code>--concurrency 10</code>, dbworkload will create as follows:</p> <ul> <li>Process-1: MainThread + 1 extra thread. Total = 2</li> <li>Process-2: MainThread + 1 extra thread. Total = 2</li> <li>Process-3: MainThread + 2 extra threads. Total = 3</li> <li>Process-4: MainThread + 2 extra threads. Total = 3</li> </ul> <p>Total executing threads/connections = 10</p> <p>This allows you to fine tune the count of Python processes and threads to fit your system.</p> <p>Furthermore, each executing thread receives a unique ID (an integer). The ID is passed to the workload class with function <code>setup()</code>, along with the total count of threads, i.e. the value passed to <code>-c/--concurrency</code>. You can leverage the ID and the thread count in various ways, for example, to have each thread process a subset of a dataset.</p>"},{"location":"#generating-csv-files","title":"Generating CSV files","text":"<ul> <li>You can seed a database quickly by letting <code>dbworkload</code> generate pseudo-random data and import it.</li> <li><code>dbworkload</code> takes the DDL as an input and creates an intermediate YAML file, with the definition of what data you want to create (a string, a number, a date, a bool..) based on the column data type.</li> <li>You then refine the YAML file to suit your needs, for example, the size of the string, a range for a date, the precision for a decimal, a choice among a discrete list of values..</li> <li>You can also specify what is the percentage of NULL for any column, or how many elements in an ARRAY type.</li> <li>You then specify the total row count, how many rows per file, and in what order, if any, to sort by.</li> <li>Then <code>dbworkload</code> will generate the data into CSV or TSV files, compress them if so requested.</li> <li>You can then optionally merge-sort the files using command <code>merge</code>.</li> </ul> <p>Write up blog: Generate multiple large sorted csv files with pseudo-random data</p> <p>Find out more on the <code>yaml</code>, <code>csv</code> and <code>merge</code> commands by running</p> <pre><code>dbworkload util --help\n</code></pre> <p>Consult file <code>workloads/postgres/bank.yaml</code> for a list of all available generators and options.</p>"},{"location":"about/","title":"Acknowledgments","text":"<p>Some methods and classes have been taken and modified from, or inspired by, https://github.com/cockroachdb/movr</p>"},{"location":"drivers/","title":"Supported DBMS drivers","text":""},{"location":"drivers/#1-psycopg-postgresql-cockroachdb","title":"1. psycopg (PostgreSQL, CockroachDB)","text":"<p>Driver documentation: Psycopg 3.</p> <pre><code># installation\npip install dbworkload[postgres]\n\n# sample use\ndbworkload run -w workloads/postgres/bank.py \\\n  --uri 'postgres://cockroach:cockroach@localhost:26257/bank?sslmode=require' \\\n  -l debug --args '{\"read_pct\":50}' -i 1 -c 1\n</code></pre>"},{"location":"drivers/#2-mysql-connector-python-mysql-tidb-singlestore","title":"2. mysql-connector-python (MySQL, TiDB, Singlestore)","text":"<p>Driver documentation: MySQL Connector/Python Developer Guide.</p> <pre><code># installation\npip3 install dbworkload[mysql]\n\n# sample use\ndbworkload run -w workloads/mysql/bank.py \\\n  --uri 'user=root,password=London123,host=localhost,port=3306,database=bank,client_flags=SSL' \\\n   --driver mysql --args '{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#3-mariadb-mariadb","title":"3. mariadb (MariaDB)","text":"<p>Driver documentation: MariaDB Connector/Python.</p> <pre><code># installation\npip3 install dbworkload[mariadb]\n\n# sample use\ndbworkload run -w workloads/mariadb/bank.py \\\n  --uri 'user=user1,password=password1,host=localhost,port=3306,database=bank' \\\n  --driver maria --args '{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#4-oracledb-oracle","title":"4. oracledb (Oracle)","text":"<p>Driver documentation: python-oracledb\u2019s documentation.</p> <pre><code># installation\npip3 install dbworkload[oracle]\n\n# sample use\ndbworkload run -w workloads/oracle/bank.py \\\n  --uri \"user=admin,password=password1,dsn=\"myhost.host2.us-east-1.rds.amazonaws.com:1521/OMS\" \\\n  --driver oracle --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#5-pyodbc-ms-sqlserver","title":"5. pyodbc (MS SQLServer)","text":"<p>Under construction...</p> <p>Driver documentation: Python SQL driver.</p> <pre><code># installation\npip3 install dbworkload[sqlserver]\n\n# sample use\ndbworkload run -w workloads/sqlserver/bank.py \\\n  --uri \"\" \\\n   --driver sqlserver --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#6-pymongo-mongodb","title":"6. pymongo (MongoDB)","text":"<p>Driver documentation: MongoDB PyMongo Documentation.</p> <pre><code># installation\npip3 install dbworkload[mongo]\n\n# sample use\ndbworkload run -w workloads/mongo/bank.py \\\n  --uri \"mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000\" \\\n  --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#7-scylla-driver-cassandra-scylladb","title":"7. scylla-driver (Cassandra, ScyllaDB)","text":"<p>Under construction...</p> <p>Driver documentation: Python Driver for Scylla and Apache Cassandra.</p> <pre><code># installation\npip3 install dbworkload[cassandra]\n\n# sample use\ndbworkload run -w workloads/cassandra/bank.py \\\n  --uri \"\" \\\n   --driver cassandra --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"software-requirements/","title":"Software Requirements","text":"<p><code>dbworkload</code> requires at least Python 3.8 and the <code>pip</code> utility, installed and upgraded.</p> <p><code>dbworkload</code> dependencies are installed automatically by the <code>pip</code> tool.</p> <p>It has run successfully on Ubuntu 20.04+, MacOSX on both Intel and Apple silicone.</p>"},{"location":"getting_started/0/","title":"Env Setup","text":"<pre><code># using ubuntu 20.04 LTS\nsudo apt update\nsudo apt install -y python3-pip\n\n# upgrade pip - must have pip version 20.3+ \npip3 install --upgrade pip\n\npip3 install dbworkload[postgres]\n\nmkdir workloads\ncd workloads\n\n# the workload class\nwget https://raw.githubusercontent.com/fabiog1901/dbworkload/main/workloads/postgres/bank.py\n\n# the DDL file\nwget https://raw.githubusercontent.com/fabiog1901/dbworkload/main/workloads/postgres/bank.sql\n\n# the data generation definition file\nwget https://raw.githubusercontent.com/fabiog1901/dbworkload/main/workloads/postgres/bank.yaml\n</code></pre>"},{"location":"getting_started/1/","title":"Init the workload","text":"<p>Make sure your CockroachDB cluster or PostgreSQL server is up and running. Please note: this guide assumes the database server and dbworkload are both runnining locally and can communicate with each other via <code>localhost</code>.</p> <p>Connect to the SQL prompt and execute the DDL statements in the <code>bank.sql</code> file. In CockroachDB, you can simply run</p> <pre><code>sql&gt; \\i bank.sql\n</code></pre> <p>Next, generate some CSV data to seed the database:</p> <pre><code>dbworkload util csv -i bank.yaml -x 1\n</code></pre> <p>The CSV files will be located inside a <code>bank</code> directory.</p> <pre><code>$ ls -lh bank\ntotal 1032\n-rw-r--r--  1 fabio  staff   513K Apr  9 13:01 ref_data.0_0_0.csv\n\n$ head -n2 bank/ref_data.0_0_0.csv \n0       simplefaker     b66ab5dc-1fcc-4ac8-8ad0-70bbbb463f00    alpha   16381   {124216.6,416559.9,355271.42,443666.45,689859.03,461510.94,31766.46,727918.45,361202.5,561364.1}        12421672576.9632        2022-10-18 04:57:37.613512      2022-10-18     13:36:48 1001010011      \\xe38a2e10b400a8e77eda  {ID-cUJeNcMZ,ID-mWxhyiqN,ID-0FnlVOO5}   0       \"{\"\"k\"\":\"\"cUJNcMZ\"\"}\"\n1       simplefaker     f2ebb78a-5af3-4755-8c22-2ad06aa3b26c    bravo   39080           35527177861.6551        2022-12-25 09:12:04.771673      2022-12-25      13:05:42        0110111101      \\x5a2efedf253aa3fbeea8  {ID-gQkRkMxIkSjihWcWTcr,ID-o7iDzl9AMJoFfduo6Hz,ID-5BS3MlZgOjxFZRBgBmf}  0       \"{\"\"k\"\":\"\"5Di0UHLWMEuR7\"\"}\"\n</code></pre> <p>Now you can import the CSV file. In CockroachDB, my favorite method is to use a webserver to serve the CSV file. Open a new terminal then start a simple python server</p> <pre><code>cd workloads\ncd bank\npython3 -m http.server 3000\n</code></pre> <p>If you open your browser at http://localhost:3000 you should see file <code>ref_data.0_0_0.csv</code> being served.</p> <p>At the SQL prompt, import the file</p> <pre><code>sql&gt; IMPORT INTO ref_data CSV DATA ('http://localhost:3000/ref_data.0_0_0.csv') WITH delimiter = e'\\t'; \n</code></pre> <p>In PostgreSQL Server, at the SQL prompt, just use <code>COPY</code></p> <pre><code>bank=# COPY ref_data FROM '/Users/fabio/workloads/bank/ref_data.0_0_0.csv' WITH CSV DELIMITER AS e'\\t';\nCOPY 100\nTime: 2.713 ms\n</code></pre>"},{"location":"getting_started/2/","title":"Run the workload","text":"<p>Run the workload using 4 connections for 120 seconds or 100k cycles, whichever comes first.</p> <pre><code># CockroachDB\ndbworkload run -w bank.py -c 4 --uri 'postgres://root@localhost:26257/bank?sslmode=disable' -d 120 -i 100000\n\n# PostgreSQL\ndbworkload run -w bank.py -c 4 --uri 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable' -d 120 -i 100000\n</code></pre> <p><code>dbworkload</code> will output rolling statistics about throughput and latency for each transaction in your workload class</p> <pre><code>  elapsed  id               threads    tot_ops    tot_ops/s    period_ops    period_ops/s    mean(ms)    p50(ms)    p90(ms)    p95(ms)    p99(ms)    max(ms)\n---------  -------------  ---------  ---------  -----------  ------------  --------------  ----------  ---------  ---------  ---------  ---------  ---------\n        8  __cycle__              4        190        23.00           190           19.00      107.20      53.90     200.14     200.71     202.06     204.84\n        8  read                   4         97        12.00            97            9.00       25.59      18.81      51.72      52.71      68.33      81.11\n        8  txn1_new               4         93        11.00            93            9.00       46.17      46.93      48.44      49.08      61.33      67.98\n        8  txn2_verify            4         93        11.00            93            9.00       76.12      83.02      84.46      84.56      95.24     102.75\n        8  txn3_finalize          4         93        11.00            93            9.00       69.99      69.03      80.65      83.21      93.16      99.95 \n[...]\n</code></pre> <p>You can always use pgAdmin for PostgreSQL Server or the DB Console for CockroachDB to view your workload, too.</p> <p>There are many built-in options. Check them out with</p> <pre><code>dbworkload --help\n</code></pre>"},{"location":"getting_started/getting-started/","title":"Getting Started","text":"<p>Example using PostgreSQL Server and CockroachDB.</p> <p>Class <code>Bank</code> in file <code>workloads/postgres/bank.py</code> is an example of one such user-created workload. The class defines 3 simple transactions that have to be executed by <code>dbworkload</code>. Have a look at the <code>bank.py</code>, <code>bank.yaml</code> and <code>bank.sql</code> in the <code>workload/postgres/</code> folder in this project.</p> <p>Head to file <code>workload/postgres/bank.sql</code> to see what the database schema look like. We have 2 tables:</p> <ul> <li>the <code>transactions</code> table, where we record the bank payment transactions.</li> <li>the <code>ref_data</code> table.</li> </ul> <p>Take a close look at this last table: each column represent a different type, which brings us to the next file.</p> <p>File <code>bank.yaml</code> is the data generation definition file. For each column of table <code>ref_data</code>, we deterministically generate random data. This file is meant as a guide to show what type of data can be generated, and what args are required.</p> <p>File <code>bank.py</code> defines the workload. The workload is defined as a class object. The class defines 2 methods: <code>loop()</code> and the constructor, <code>__init__()</code>. All other methods are part of the application logic of the workload. Read the comments along the code for more information.</p> <p>Let's run the sample Bank workload.</p>"}]}