{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dbworkload - DBMS workload utility","text":""},{"location":"#overview","title":"Overview","text":"<p>The goal of <code>dbworkload</code> is to ease the creation and execution of bespoke database workload scripts.</p> <p>The user is responsible for coding the workload logic as a Python <code>class</code>, while <code>dbworkload</code> takes care of providing ancillary features, such as configuring the workload concurrency, duration and/or iteration, and more.</p> <p>The user, by coding the class, has complete control of the workload flow: what transactions are executed in which order, and what statements are included in each transaction, plus what data to insert and how to generate it and manipulate it.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#acknowledgments","title":"Acknowledgments","text":"<p>Some methods and classes have been taken and modified from, or inspired by, https://github.com/cockroachdb/movr</p>"},{"location":"cli/","title":"CLI","text":"<p>This page provides documentation for the command line arguments and options.</p>"},{"location":"cli/#dbworkload","title":"dbworkload","text":"<p>dbworkload v0.5.1: DBMS workload utility.</p> <p>Usage:</p> <pre><code>dbworkload [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -v, --version         Print the version and exit\n  --install-completion  Install completion for the current shell.\n  --show-completion     Show completion for the current shell, to copy it or\n                        customize the installation.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli/#run","title":"run","text":"<p>Run the workload.</p> <p>Usage:</p> <pre><code>dbworkload run [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -w, --workload FILE             Filepath to the workload module.\n  --driver [postgres|mysql|maria|oracle|sqlserver|mongo|cassandra]\n                                  DBMS driver.\n  --uri TEXT                      The connection URI to the database.\n  -x, --procs INTEGER             Number of processes to spawn. Defaults to\n                                  &lt;system-cpu-count&gt;.\n  --args TEXT                     JSON string, or filepath to a JSON/YAML\n                                  file, to pass to Workload.\n  -c, --concurrency INTEGER       Number of concurrent workers.  [default: 1]\n  -r, --ramp INTEGER              Ramp up time in seconds.  [default: 0]\n  -i, --iterations INTEGER        Total number of iterations. Defaults to &lt;ad\n                                  infinitum&gt;.\n  -d, --duration INTEGER          Duration in seconds. Defaults to &lt;ad\n                                  infinitum&gt;.\n  -k, --conn-duration INTEGER     The number of seconds to keep database\n                                  connection alive before restarting. Defaults\n                                  to &lt;ad infinitum&gt;.\n  -a, --app-name TEXT             The application name specified by the\n                                  client. Defaults to &lt;db-name&gt;.\n  --no-autocommit                 Unset autocommit in the connections.\n  -p, --port INTEGER              The port of the Prometheus server.\n                                  [default: 26260]\n  -q, --quiet                     Disable printing intermediate stats.\n  -s, --save                      Save stats to CSV files.\n  -l, --log-level [debug|info|warning|error]\n                                  Set the logging level.  [default: info]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#util","title":"util","text":"<p>Various utils.</p> <p>Usage:</p> <pre><code>dbworkload util [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#csv","title":"csv","text":"<p>Generate CSV files from a YAML data generation file.</p> <p>Usage:</p> <pre><code>dbworkload util csv [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE                Filepath to the YAML data generation file.\n                                  [required]\n  -o, --output DIRECTORY          Output directory for the CSV files. Defaults\n                                  to &lt;input-basename&gt;.\n  -x, --procs INTEGER             Number of processes to spawn. Defaults to\n                                  &lt;system-cpu-count&gt;.\n  --csv-max-rows INTEGER          Max count of rows per resulting CSV file.\n                                  [default: 100000]\n  -n, --hostname TEXT             The hostname of the http server that serves\n                                  the CSV files.\n  -p, --port INTEGER              The port of the http server that servers the\n                                  CSV files.  [default: 3000]\n  -c, --compression [bz2|gzip|xz|zip]\n                                  The compression format.\n  -d, --delimiter TEXT            The delimeter char to use for the CSV files.\n                                  Defaults to \"tab\".\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#gen_stub","title":"gen_stub","text":"<p>Generate a dbworkload class stub.</p> <p>Usage:</p> <pre><code>dbworkload util gen_stub [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE  Input SQL file  [required]\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli/#html","title":"html","text":"<p>Save charts to HTML from the dbworkload statistics CSV file.</p> <p>Usage:</p> <pre><code>dbworkload util html [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE  Input CSV file  [required]\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli/#merge","title":"merge","text":"<p>Merge multiple sorted CSV files into 1+ files.</p> <p>Usage:</p> <pre><code>dbworkload util merge [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input DIRECTORY   Directory of files to be merged  [required]\n  -o, --output PATH       Output filepath. Defaults to &lt;input&gt;.merged.\n  --csv-max-rows INTEGER  Max count of rows per resulting CSV file.  [default:\n                          100000]\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"cli/#merge_csvs","title":"merge_csvs","text":"<p>Merge multiple dbworkload statistic CSV files.</p> <p>Usage:</p> <pre><code>dbworkload util merge_csvs [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input_dir DIRECTORY  Input CSV directory  [required]\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli/#plot","title":"plot","text":"<p>Plot charts from the dbworkload statistics CSV file.</p> <p>Usage:</p> <pre><code>dbworkload util plot [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE  Input CSV file  [required]\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli/#yaml","title":"yaml","text":"<p>Generate YAML data generation file from a DDL SQL file.</p> <p>Usage:</p> <pre><code>dbworkload util yaml [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE   Filepath to the DDL SQL file.  [required]\n  -o, --output PATH  Output filepath. Defaults to &lt;input-basename&gt;.yaml.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"drivers/","title":"Drivers","text":"<p>Here is the list of the currently supported drivers.</p>"},{"location":"drivers/#postgres","title":"postgres","text":"<p>For technologies such as PostgreSQL, CockroachDB</p> <p>Driver documentation: Psycopg 3.</p> <pre><code># installation\npip install dbworkload[postgres]\n\n# sample use\ndbworkload run -w workloads/postgres/bank.py \\\n  --uri 'postgres://cockroach:cockroach@localhost:26257/bank?sslmode=require' \\\n  -l debug --args '{\"read_pct\":50}' -i 1 -c 1\n</code></pre>"},{"location":"drivers/#mysql","title":"mysql","text":"<p>For technologies such as MySQL, TiDB, Singlestore</p> <p>Driver documentation: MySQL Connector/Python Developer Guide.</p> <pre><code># installation\npip3 install dbworkload[mysql]\n\n# sample use\ndbworkload run -w workloads/mysql/bank.py \\\n  --uri 'user=root,password=London123,host=localhost,port=3306,database=bank,client_flags=SSL' \\\n   --driver mysql --args '{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#mariadb","title":"mariadb","text":"<p>Driver documentation: MariaDB Connector/Python.</p> <pre><code># installation\npip3 install dbworkload[mariadb]\n\n# sample use\ndbworkload run -w workloads/mariadb/bank.py \\\n  --uri 'user=user1,password=password1,host=localhost,port=3306,database=bank' \\\n  --driver maria --args '{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#oracle","title":"oracle","text":"<p>Driver documentation: python-oracledb\u2019s documentation.</p> <pre><code># installation\npip3 install dbworkload[oracle]\n\n# sample use\ndbworkload run -w workloads/oracle/bank.py \\\n  --uri \"user=admin,password=password1,dsn=\"myhost.host2.us-east-1.rds.amazonaws.com:1521/OMS\" \\\n  --driver oracle --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#sqlserver","title":"sqlserver","text":"<p>Under construction...</p> <p>Driver documentation: Python SQL driver.</p> <pre><code># installation\npip3 install dbworkload[sqlserver]\n\n# sample use\ndbworkload run -w workloads/sqlserver/bank.py \\\n  --uri \"\" \\\n   --driver sqlserver --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#mongo","title":"mongo","text":"<p>Driver documentation: MongoDB PyMongo Documentation.</p> <pre><code># installation\npip3 install dbworkload[mongo]\n\n# sample use\ndbworkload run -w workloads/mongo/bank.py \\\n  --uri \"mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000\" \\\n  --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#scylla-driver","title":"scylla-driver","text":"<p>For technologies such as Cassandra, ScyllaDB</p> <p>Under construction...</p> <p>Driver documentation: Python Driver for Scylla and Apache Cassandra.</p> <pre><code># installation\npip3 install dbworkload[cassandra]\n\n# sample use\ndbworkload run -w workloads/cassandra/bank.py \\\n  --uri \"\" \\\n   --driver cassandra --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"how_it_works/","title":"How it works","text":"<p>It\u2019s helpful to understand first what <code>dbworkload</code> does:</p> <ul> <li> <p>At runtime, <code>dbworkload</code> first imports the class you pass, <code>bank.py</code>.</p> </li> <li> <p>It spawns n threads for concurrent execution (see next section on Concurrency).</p> </li> <li>By default, it sets the connection to <code>autocommit</code> mode.</li> <li>Each thread creates a database connection - no need for a connection pool.</li> <li>In a loop, each <code>dbworkload</code> thread will:</li> <li>execute function <code>loop()</code> which returns a list of functions.</li> <li>execute each function in the list sequentially. Each function, typically, executes a SQL statement/transaction.</li> <li>Execution stats are funneled back to the MainThread, which aggregates and, optionally, prints them to stdout and saves them to a CSV file.</li> <li>If the connection drops, it will recreate it. You can also program how long you want the connection to last.</li> <li><code>dbworkload</code> stops once a limit has been reached (iteration/duration), or you Ctrl+C.</li> </ul>"},{"location":"how_it_works/#concurrency-processes-and-threads","title":"Concurrency - processes and threads","text":"<p><code>dbworkload</code> uses both the <code>multiprocessing</code> and <code>threading</code> library to achieve high concurrency, that is, opening multiple connections to the DBMS.</p> <p>There are 2 parameters that can be used to configure how many processes you want to create, and for each process, how many threads:</p> <ul> <li><code>--procs/-x</code>, to configure the count of processes (defaults to the CPU count)</li> <li><code>--concurrency/-c</code>, to configure the total number of connections (also referred to as executing threads)</li> </ul> <p><code>dbworkload</code> will spread the load across the processes, so that each process has an even amount of threads.</p> <p>Example: if we set <code>--procs 4</code> and <code>--concurrency 10</code>, dbworkload will create as follows:</p> <ul> <li>Process-1: MainThread + 1 extra thread. Total = 2</li> <li>Process-2: MainThread + 1 extra thread. Total = 2</li> <li>Process-3: MainThread + 2 extra threads. Total = 3</li> <li>Process-4: MainThread + 2 extra threads. Total = 3</li> </ul> <p>Total executing threads/connections = 10</p> <p>This allows you to fine tune the count of Python processes and threads to fit your system.</p> <p>Furthermore, each executing thread receives a unique ID (an integer). The ID is passed to the workload class with function <code>setup()</code>, along with the total count of threads, i.e. the value passed to <code>-c/--concurrency</code>. You can leverage the ID and the thread count in various ways, for example, to have each thread process a subset of a dataset.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#software-requirements","title":"Software Requirements","text":"<p><code>dbworkload</code> requires at least Python 3.8 and the <code>pip</code> utility, installed and upgraded.</p> <p><code>dbworkload</code> dependencies are installed automatically by the <code>pip</code> tool.</p> <p>It has run successfully on Ubuntu 20.04+, MacOSX on both Intel and Apple silicone.</p>"},{"location":"installation/#dbworkload-installation","title":"dbworkload installation","text":"<p><code>dbworkload</code> comes already pre-packaged, available from PyPI.</p> <p>Generally, you want to specify which of the supported drivers you want to install.</p> <p>In below example, we install with the Psycopg3 driver, so we run</p> <pre><code>pip3 install dbworkload[postgres]\n</code></pre> <p>Confirm installation is successful by running</p> <pre><code>$ dbworkload --version\ndbworkload : 0.5.1\nPython     : 3.11.3\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>The best way to understand how <code>dbworkload</code> works is by running through a good example.</p> <p>We will be using PostgreSQL Server and CockroachDB, but the same logic can be applied to any of the other supported technologies.</p> <p>In this tutorial, we will go through the following tasks:</p> <ol> <li>As a prerequisite, we setup our working environment.</li> <li>We start with the DDL of a few tables and the SQL statements that are routinely executed against these tables.</li> <li>We generate some random datasets for seeding the database tables.</li> <li>We then create the tables in our database, and import the generated dataset.</li> <li>From the SQL statements, we create the <code>dbworload</code> class file, that is, our workload file.</li> <li>We run the workload, saving stats to a CSV file.</li> <li>We collect the information and plot a chart to display the results of the test run.</li> </ol>"},{"location":"getting_started/0/","title":"Preparation","text":"<p>For this tutorial, we assume you have a CockroachDB or PostgreSQL Server running locally and accessible at <code>localhost</code>.</p> <p>Make sure <code>dbworkload</code> is installed on your machine, see how at Installation.</p> <p>Create a directory <code>workloads</code> to store all our files.</p> <pre><code>mkdir workloads\ncd workloads\n</code></pre> <p>For reference, here are the URI used to connect to my local instances</p>"},{"location":"getting_started/0/#postgresql","title":"PostgreSQL","text":"<pre><code>$ psql 'postgres://fabio:postgres@localhost:5432/postgres?sslmode=disable'\nTiming is on.\npsql (16.2)\nType \"help\" for help.\n\npostgres=# select version();\n                                                              version                                                               \n------------------------------------------------------------------------------------------------------------------------------------\n PostgreSQL 16.2 (Postgres.app) on aarch64-apple-darwin21.6.0, compiled by Apple clang version 14.0.0 (clang-1400.0.29.102), 64-bit\n(1 row)\n\nTime: 1.926 ms\npostgres=# \n</code></pre>"},{"location":"getting_started/0/#cockroachdb","title":"CockroachDB","text":"<pre><code>$ cockroach sql --url 'postgres://cockroach:cockroach@localhost:26257/defaultdb?sslmode=require'\n#\n# Welcome to the CockroachDB SQL shell.\n# All statements must be terminated by a semicolon.\n# To exit, type: \\q.\n#\n# Server version: CockroachDB CCL v24.2.3 (x86_64-apple-darwin19, built 2024/09/23 22:30:57, go1.22.5 X:nocoverageredesign) (same version as client)\n# Cluster ID: e360faa9-2ba3-4e92-bd51-fc7e88cf24a8\n# Organization: Workshop\n#\n# Enter \\? for a brief introduction.\n#\ncockroach@localhost:26257/defaultdb&gt; select version();\n                                                   version\n-------------------------------------------------------------------------------------------------------------\n  CockroachDB CCL v24.2.3 (x86_64-apple-darwin19, built 2024/09/23 22:30:57, go1.22.5 X:nocoverageredesign)\n(1 row)\n\nTime: 1ms total (execution 1ms / network 0ms)\n\ncockroach@localhost:26257/defaultdb&gt;\n</code></pre>"},{"location":"getting_started/1/","title":"Schema and SQL statements","text":"<p>Our starting point is the schema and the SQL statements we want to eventually emulate.</p>"},{"location":"getting_started/1/#ddl","title":"DDL","text":"<p>For this tutorial, the DDL is very simple, just 2 tables.</p> <pre><code>-- file: bank.ddl\n\nCREATE TABLE ref_data (\n    acc_no BIGINT PRIMARY KEY,\n    external_ref_id UUID,\n    created_time TIMESTAMPTZ,\n    acc_details VARCHAR\n);\n\nCREATE TABLE orders (\n    acc_no BIGINT NOT NULL,\n    id UUID NOT NULL default gen_random_uuid(),\n    status VARCHAR NOT NULL,\n    amount DECIMAL(15, 2),\n    ts TIMESTAMPTZ default now(),\n    CONSTRAINT pk PRIMARY KEY (acc_no, id)\n);\n</code></pre> <p>Note</p> <p>PostgreSQL and CockroachDB share the same SQL syntax as CockroachDB has adopted the postgres wire API protocol. This means the above <code>CREATE TABLE</code> statements work for both technologies \ud83d\ude80</p> <p>Save the DDL to a file <code>bank.ddl</code>.</p>"},{"location":"getting_started/1/#statements","title":"Statements","text":"<p>This below is the list of statements that constitute our workload. Bind parameters, denoted with <code>%s</code>, will vary, and we are expected to simulate them.</p> <pre><code>-- file: bank.sql\n\n-- read operation, executed 50% of the time\nSELECT * FROM orders WHERE acc_no = %s AND id = %s;\n\n-- below 2 transactions constitute a complete order execution\n\n-- new_order\nINSERT INTO orders (acc_no, status, amount) VALUES (%s, 'Pending', %s) RETURNING id;\n\n-- execute order - this is an explicit transaction\nSELECT * FROM ref_data WHERE acc_no = %s;\nUPDATE orders SET status = 'Complete' WHERE (acc_no, id) = (%s, %s);\n</code></pre> <p>Save this to file <code>bank.sql</code></p> <p>A quick note</p> <p>For this exercise, we will not spend too much time caring about what the statements do. The statements in this example will not make much sense, so just focus on <code>dbworkload</code> \ud83d\ude42</p>"},{"location":"getting_started/2/","title":"Seed the database tables","text":""},{"location":"getting_started/2/#create-the-tables","title":"Create the tables","text":"<p>Create the tables in a database called <code>bank</code>.</p>"},{"location":"getting_started/2/#postgresql","title":"PostgreSQL","text":"<pre><code>postgres=# CREATE DATABASE bank;\n</code></pre> <p>Now, you need to disconnect and reconnect to <code>bank</code></p> <pre><code>psql 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable'\n</code></pre> <p>Once on the SQL prompt, import the file, or copy-paste, as you prefer.</p> <pre><code>bank=# \\i bank.ddl\n\nbank=# \\d\n--          List of relations\n--  Schema |   Name   | Type  | Owner \n-- --------+----------+-------+-------\n--  public | orders   | table | fabio\n--  public | ref_data | table | fabio\n-- (2 rows)\n</code></pre>"},{"location":"getting_started/2/#cockroachdb","title":"CockroachDB","text":"<pre><code>defaultdb&gt; CREATE DATABASE bank;\n\ndefaultdb&gt; USE bank;\n\nbank&gt; \\i bank.ddl\n\nbank&gt; SHOW TABLES;\n--   schema_name | table_name | type  |   owner   | estimated_row_count | locality\n-- --------------+------------+-------+-----------+---------------------+-----------\n--   public      | orders     | table | cockroach |                   0 | NULL\n--   public      | ref_data   | table | cockroach |                   0 | NULL\n-- (2 rows)\n</code></pre>"},{"location":"getting_started/2/#generate-datasets","title":"Generate datasets","text":"<p>Next, generate some CSV data to seed the database. <code>dbworkload</code> has 2 built-in utility functions that can assist with this task:</p> <ul> <li><code>dbworkload util yaml</code>, which converts a DDL file into a data generation definition file, structured in YAML.</li> <li><code>dbworkload util csv</code>, which takes the YAML file as input and outputs CSV files.</li> </ul> <p>Read the docs for util commands <code>yaml</code> and <code>csv</code> for more information.</p> <p>Let's use the <code>yaml</code> utility with our <code>bank.ddl</code> file.</p> <pre><code>dbworkload util yaml -i bank.ddl\n</code></pre> <p>For this exercise, we will use below simplified YAML file. Replace the content of <code>bank.yaml</code> with below YAML</p> <pre><code>ref_data:\n- count: 1000\n  sort-by: \n    - acc_no\n  columns:\n    acc_no:\n      type: sequence\n      args:\n        start: 0\n    external_ref_id:\n      type: UUIDv4\n      args: {}\n    created_time:\n      type: timestamp\n      args:\n        start: '2000-01-01'\n        end: '2024-12-31'\n        format: '%Y-%m-%d %H:%M:%S.%f'\n    acc_details:\n      type: string\n      args: \n        min: 10\n        max: 30\n</code></pre> <p>Now let's create a CSV dataset</p> <pre><code>dbworkload util csv -i bank.yaml -x 1\n</code></pre> <p>The CSV files will be located inside a <code>bank</code> directory.</p> <pre><code>$ ls -lh1 bank\nref_data.0_0_0.tsv\n\n$ wc -l bank/*\n    1000 bank/ref_data.0_0_0.tsv\n</code></pre> <p>Inspect it</p> <pre><code>$ head -n5 bank/ref_data.0_0_0.tsv \n0       3a2edc9d-a96b-4541-99ae-0098527545f7    2008-03-19 06:20:27.209214      CWUh0FWashpmWCx4LF3kb1\n1       829de6d6-103c-4707-9668-c4359ef5373c    2014-02-13 22:04:20.168239      QGspICZBHYpRLnHNcg\n2       5dd183af-d728-4e12-8b11-2900b6f6880a    2019-04-01 16:14:40.388236      sEUukccOePdnIbiQyVUSi0HS7rL\n3       21f00778-5fca-4302-8380-56fa461adfc8    2003-05-21 19:21:21.598455      OQTNwxoZIAdNmcA6fJM5eGDvMJgKJ\n4       035dac61-b4a3-40a4-9e4d-0deb50fef3ae    2011-08-15 06:15:40.405698      RvToVnn20BEXoxFzw9QFpCt\n</code></pre>"},{"location":"getting_started/2/#importing-datasets","title":"Importing datasets","text":"<p>Now we are ready to import the CSV file into our table <code>ref_data</code>.</p>"},{"location":"getting_started/2/#postgresql_1","title":"PostgreSQL","text":"<p>For PostgreSQL Server, at the SQL prompt, just use <code>COPY</code></p> <pre><code>bank=# COPY ref_data FROM '/path/to/workloads/bank/ref_data.0_0_0.csv' WITH CSV DELIMITER AS e'\\t';\nCOPY 1000\nTime: 8.713 ms\n</code></pre>"},{"location":"getting_started/2/#cockroachdb_1","title":"CockroachDB","text":"<p>For CockroachDB, my favorite method is to use a webserver for serving the CSV files.</p> <p>Open a new terminal then start a simple python server</p> <pre><code>cd workloads/bank\npython3 -m http.server 3000\n</code></pre> <p>If you open your browser at http://localhost:3000, you should see file <code>ref_data.0_0_0.tsv</code> being served.</p> <p>At the SQL prompt, import the file</p> <pre><code>bank&gt; IMPORT INTO ref_data CSV DATA ('http://localhost:3000/ref_data.0_0_0.tsv') WITH delimiter = e'\\t', nullif = '';\n        job_id        |  status   | fraction_completed | rows | index_entries | bytes\n----------------------+-----------+--------------------+------+---------------+--------\n  1013454367369822209 | succeeded |                  1 | 1000 |             0 | 71401\n(1 row)\n</code></pre>"},{"location":"getting_started/2/#review-imported-data","title":"Review imported data","text":"<p>Good stuff, your dataset has been successfully imported, confirm on the SQL prompt</p> <pre><code>bank=# SELECT * FROM ref_data LIMIT 5;\n acc_no |           external_ref_id            |         created_time          |       acc_details        \n--------+--------------------------------------+-------------------------------+--------------------------\n      0 | a4a689d3-ae12-4ca2-8b74-a9929c89f420 | 2015-06-03 04:03:48.248701-04 | DiKysAoJeZ9hqDehOhE2N\n      1 | 01abe15c-fcae-4492-812e-37b2212ababa | 2019-06-11 17:57:05.76814-04  | nEYyCwhTJTXXaMfeHeWj\n      2 | 40cc3ccd-74bf-4688-ae85-7f68cd0e44f2 | 2001-10-10 18:42:21.865742-04 | DFhQ0aUbjqJ9CsczdyWv\n      3 | 9491509e-0424-4cdf-879f-790cff2d7289 | 2003-08-13 20:26:49.484942-04 | ggpx35EPH698N2MGlUPV13P0\n      4 | a54b9e47-10c7-4dce-9db8-12c7eaf80e17 | 2022-06-02 22:56:20.445508-04 | JCOhKbB3YMrOOBm\n(5 rows)\n\nbank=# SELECT count(*) FROM ref_data;\n count \n-------\n  1000\n(1 row)\n</code></pre> <p>We're then ready to proceed to the next section.</p>"},{"location":"getting_started/3/","title":"Create the workload class","text":"<p>We now create the Python class file that describes our workload.</p> <p>This is going to be a regular <code>.py</code> file which we need to create from scratch.</p> <p>Fortunately, we can use built-in function <code>dbworkload util gen_stub</code> to generate a skeleton which we can use as a base to get started.</p> <p>Read the helpful doc page for the gen_stub command.</p>"},{"location":"getting_started/3/#create-the-stub-file","title":"Create the stub file","text":"<p>Execute the following command</p> <pre><code>dbworkload util gen_stub -i bank.sql\n</code></pre> <p>A new file, <code>bank.py</code>, will be created in your directory.</p>"},{"location":"getting_started/3/#review-the-workload-class-file","title":"Review the workload class file","text":"<p>The file is pretty long to be pasted here all in one shot, so we will go through it in sections.</p>"},{"location":"getting_started/3/#library-imports","title":"Library imports","text":"<p>The first few lines define the imports</p> <pre><code>import datetime as dt\nimport psycopg\nimport random\nimport time\nfrom uuid import uuid4\n</code></pre> <p>These are the most common libraries used when using a workload class. More can be added as needed.</p>"},{"location":"getting_started/3/#class-init","title":"Class init","text":"<p>Here is the definition of class <code>Bank</code>.</p> <pre><code>class Bank:\n    def __init__(self, args: dict):\n        # args is a dict of string passed with the --args flag\n\n        self.think_time: float = float(args.get(\"think_time\", 5) / 1000)\n\n        # you can arbitrarely add any variables you want\n        self.my_var = 1\n\n        # translation table for efficiently generating a string\n        # -------------------------------------------------------\n        # make translation table from 0..255 to A..Z, 0..9, a..z\n        # the length must be 256\n        self.tbl = bytes.maketrans(\n            bytearray(range(256)),\n            bytearray(\n                [ord(b\"a\") + b % 26 for b in range(113)]\n                + [ord(b\"0\") + b % 10 for b in range(30)]\n                + [ord(b\"A\") + b % 26 for b in range(113)]\n            ),\n        )\n</code></pre> <p>The <code>init</code> adds some convenient examples on how to pass runtime arguments and set class variables. <code>self.tbl</code> is used to generate random string - we will review that in the next sections.</p>"},{"location":"getting_started/3/#setup","title":"setup()","text":"<p>Next up is <code>setup()</code>. <code>dbworkload</code> invokes this function when it first starts up.</p> <pre><code>    # the setup() function is executed only once\n    # when a new executing thread is started.\n    # Also, the function is a vector to receive the excuting threads's unique id and the total thread count\n    def setup(self, conn: psycopg.Connection, id: int, total_thread_count: int):\n        with conn.cursor() as cur:\n            print(\n                f\"My thread ID is {id}. The total count of threads is {total_thread_count}\"\n            )\n            print(cur.execute(f\"select version()\").fetchone()[0])\n</code></pre>"},{"location":"getting_started/3/#loop","title":"loop()","text":"<p>Function <code>loop()</code> is what is repeatedly executed by <code>dbworkload</code>. In the return list you define the functions you want to execute.</p> <pre><code>    # the loop() function returns a list of functions\n    # that dbworkload will execute, sequentially.\n    # Once every func has been executed, loop() is re-evaluated.\n    # This process continues until dbworkload exits.\n    def loop(self):\n        return [\n            self.txn_0,\n            self.txn_1,\n            self.txn_2,\n            self.txn_3,\n        ]\n</code></pre>"},{"location":"getting_started/3/#utility-functions","title":"Utility functions","text":"<p>Here the stub includes some commonly used functions, such as <code>random_str</code> to generate just that.</p> <pre><code>    #####################\n    # Utility Functions #\n    #####################\n    def __think__(self, conn: psycopg.Connection):\n        time.sleep(self.think_time)\n\n    def random_str(self, size: int = 12):\n        return (\n            random.getrandbits(8 * size)\n            .to_bytes(size, \"big\")\n            .translate(self.tbl)\n            .decode()\n        )\n</code></pre>"},{"location":"getting_started/3/#workload-transactions","title":"Workload transactions","text":"<p>Next are the stub of the functions generated from the SQL statements in the <code>bank.sql</code> file.</p> <p>This is the read operation</p> <pre><code>    def txn_0(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT *\n                FROM orders\n                WHERE acc_no = %s\n                  AND id = %s\n                \"\"\",\n                (\n                    # add bind parameter,\n                    # add bind parameter, \n                ), \n            ).fetchall()\n</code></pre> <p>And below are the transactions related to the order</p> <pre><code>    def txn_1(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                INSERT INTO orders (acc_no, status, amount)\n                VALUES (%s, 'Pending', %s) RETURNING id\n                \"\"\",\n                (\n                    # add bind parameter,\n                    # add bind parameter, \n                ), \n            ).fetchall()\n\n    def txn_2(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT *\n                FROM ref_data\n                WHERE acc_no = %s\n                \"\"\",\n                (\n                    # add bind parameter, \n                ), \n            ).fetchall()\n\n    def txn_3(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                UPDATE orders\n                SET status = 'Complete'\n                WHERE (acc_no,\n                       id) = (%s,\n                              %s)\n                \"\"\",\n                (\n                    # add bind parameter,\n                    # add bind parameter, \n                ), \n            )\n</code></pre>"},{"location":"getting_started/3/#helpful-tips","title":"Helpful tips","text":"<p>Finally, the stub finishes with some tips on how to generate random data</p> <pre><code>'''\n# Quick random generators reminder\n\n# random string of 25 chars\nself.random_str(25),\n\n# random int between 0 and 100k\nrandom.randint(0, 100000),\n\n# random float with 2 decimals \nround(random.random()*1000000, 2)\n\n# now()\ndt.datetime.utcnow()\n\n# random timestamptz between certain dates,\n# expressed as unix ts\ndt.datetime.fromtimestamp(random.randint(1655032268, 1759232268))\n\n# random UUID\nuuid4()\n\n# random bytes\nsize = 12\nrandom.getrandbits(8 * size).to_bytes(size, \"big\")\n\n'''\n</code></pre> <p>In the next session, we customize the file to fit our workload.</p> <p>Psycopg 3 basic usage</p> <p>It might be a good idea now to refresh how the <code>psycopg</code> driver works. Fortunately, there is a great intro doc in the official website \ud83d\ude80</p>"},{"location":"getting_started/4/","title":"Edit the workload","text":"<p>Ok, we have only a few changes to make to our stub file so that it matches our intended workload.</p>"},{"location":"getting_started/4/#read-transaction","title":"Read Transaction","text":"<p>The SELECT statement requires us to pass <code>acc_no</code> and <code>id</code>.</p> <p><code>acc_no</code> is a number between 0 and 999. This matches the data we generated for table <code>ref_data</code>.</p> <p><code>id</code> is a UUID and it is supposed to be generated by the database upon insertion.</p> <p>So how can we pass a valid <code>(acc_no, id)</code> to our query? We can keep track of, say, 10,000 values (tuples) into a list, and pick from it randomly. The list will be populated as we insert new orders into the database, so after a few 1000's iterations, our list will be full and the SELECT will pass existing values.</p> <p>A convenient Python object that can help is the deque, which allows us to define a max length.</p> <p>We also need a way to control how many read transactions we want to do compared to order transactions. For this, we pass an argument at runtime.</p>"},{"location":"getting_started/4/#add-required-library","title":"Add required library","text":"<pre><code>import datetime as dt\nimport psycopg\nimport random\nimport time\nfrom uuid import uuid4\nfrom collections import deque\n</code></pre>"},{"location":"getting_started/4/#add-variables-and-runtime-argument","title":"Add variables and runtime argument","text":"<p>We add here the runtime argument <code>read_pct</code> and 2 variables:</p> <ul> <li>the deque object called <code>order_tuples</code></li> <li>the <code>account_number</code> and <code>id</code> needed for the Order transaction.</li> </ul> <pre><code>class Bank:\n    def __init__(self, args: dict):\n        # args is a dict of string passed with the --args flag\n\n        self.think_time: float = float(args.get(\"think_time\", 5) / 1000)\n\n        # Runtime argument to control read vs order txns\n        # Percentage of read operations compared to order operations\n        self.read_pct: float = float(args.get(\"read_pct\", 50) / 100)\n\n        # initiate deque with a random tuple so a read won't fail\n        self.order_tuples = deque([(0, uuid4())], maxlen=10000)\n\n        # keep track of the current account number and id\n        self.account_number = 0\n        self.id = uuid4()\n\n        # translation table for efficiently generating a string\n        # -------------------------------------------------------\n        # make translation table from 0..255 to A..Z, 0..9, a..z\n        # the length must be 256\n        self.tbl = bytes.maketrans(\n            bytearray(range(256)),\n            bytearray(\n                [ord(b\"a\") + b % 26 for b in range(113)]\n                + [ord(b\"0\") + b % 10 for b in range(30)]\n                + [ord(b\"A\") + b % 26 for b in range(113)]\n            ),\n        )\n</code></pre>"},{"location":"getting_started/4/#add-bind-parameters-to-read-function","title":"Add bind parameters to read function","text":"<p>We rename the function to something more descriptive</p> <pre><code>    def txn_read(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT *\n                FROM orders\n                WHERE acc_no = %s\n                  AND id = %s\n                \"\"\",\n                random.choice(self.order_tuples),  \n            ).fetchall()\n</code></pre> <p>That's it! Our read operation is complete \ud83d\ude80.</p>"},{"location":"getting_started/4/#order-transaction","title":"Order transaction","text":"<p>The order transaction is made up of 2 distinct database transactions:</p> <ul> <li>new_order</li> <li>order_exec</li> </ul>"},{"location":"getting_started/4/#new-order","title":"New Order","text":"<p>This is the first of the 2 transactions of our Order workload.</p> <pre><code>    def txn_new_order(self, conn: psycopg.Connection):\n\n        # generate a random account number to be used for\n        # for the order transaction\n        self.account_number = random.randint(0, 999)\n\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                INSERT INTO orders (acc_no, status, amount)\n                VALUES (%s, 'Pending', %s) RETURNING id\n                \"\"\",\n                (\n                    self.account_number,\n                    round(random.random() * 1000000, 2),\n                ),\n            )\n\n            # save the id that the server generated\n            self.id = cur.fetchone()[0]\n\n            # save the (acc_no, id) tuple to our deque list\n            # for future read transactions\n            self.order_tuples.append((self.account_number, self.id))\n</code></pre>"},{"location":"getting_started/4/#order-execution","title":"Order Execution","text":"<p>The order execution transaction is an explicit transaction.</p> <pre><code>    def txn_order_exec(self, conn: psycopg.Connection):\n\n        # this is how you start an explicit transaction with psycopg\n        with conn.transaction() as tx:\n            with conn.cursor() as cur:\n                cur.execute(\n                    \"\"\"\n                    SELECT *\n                    FROM ref_data\n                    WHERE acc_no = %s\n                    \"\"\",\n                    (\n                        self.account_number,\n                    ),\n                ).fetchall()\n\n                # simulate microservice doing something...\n                time.sleep(0.02)\n\n                cur.execute(\n                    \"\"\"\n                    UPDATE orders\n                    SET status = 'Complete'\n                    WHERE \n                        (acc_no, id) = (%s, %s)\n                    \"\"\",\n                    (\n                        self.account_number,\n                        self.id,\n                    ),\n                )\n</code></pre>"},{"location":"getting_started/4/#update-loop","title":"Update <code>loop()</code>","text":"<p>The final change is to update the <code>loop()</code> function to return either a read txn or an order txn.</p> <pre><code>    def loop(self):\n        if random.random() &lt; self.read_pct:\n            return [self.txn_read]\n        return [self.txn_new_order, self.txn_order_exec]\n</code></pre>"},{"location":"getting_started/4/#full-class-file-bankpy","title":"Full class file <code>bank.py</code>","text":"<p>For completeness, here is the full edited file, minus the helpful tips</p> <pre><code>import datetime as dt\nimport psycopg\nimport random\nimport time\nfrom uuid import uuid4\nfrom collections import deque\n\n\nclass Bank:\n    def __init__(self, args: dict):\n        # args is a dict of string passed with the --args flag\n\n        self.think_time: float = float(args.get(\"think_time\", 5) / 1000)\n\n        # Percentage of read operations compared to order operations\n        self.read_pct: float = float(args.get(\"read_pct\", 0) / 100)\n\n        # initiate deque with 1 random UUID so a read won't fail\n        self.order_tuples = deque([(0, uuid4())], maxlen=10000)\n\n        # keep track of the current account number and id\n        self.account_number = 0\n        self.id = uuid4()\n\n        # translation table for efficiently generating a string\n        # -------------------------------------------------------\n        # make translation table from 0..255 to A..Z, 0..9, a..z\n        # the length must be 256\n        self.tbl = bytes.maketrans(\n            bytearray(range(256)),\n            bytearray(\n                [ord(b\"a\") + b % 26 for b in range(113)]\n                + [ord(b\"0\") + b % 10 for b in range(30)]\n                + [ord(b\"A\") + b % 26 for b in range(113)]\n            ),\n        )\n\n    # the setup() function is executed only once\n    # when a new executing thread is started.\n    # Also, the function is a vector to receive the excuting threads's unique id and the total thread count\n    def setup(self, conn: psycopg.Connection, id: int, total_thread_count: int):\n        with conn.cursor() as cur:\n            print(\n                f\"My thread ID is {id}. The total count of threads is {total_thread_count}\"\n            )\n            print(cur.execute(f\"select version()\").fetchone()[0])\n\n    # the loop() function returns a list of functions\n    # that dbworkload will execute, sequentially.\n    # Once every func has been executed, loop() is re-evaluated.\n    # This process continues until dbworkload exits.\n    def loop(self):\n        if random.random() &lt; self.read_pct:\n            return [self.txn_read]\n        return [self.txn_new_order, self.txn_order_exec]\n\n    #####################\n    # Utility Functions #\n    #####################\n    def __think__(self, conn: psycopg.Connection):\n        time.sleep(self.think_time)\n\n    def random_str(self, size: int = 12):\n        return (\n            random.getrandbits(8 * size)\n            .to_bytes(size, \"big\")\n            .translate(self.tbl)\n            .decode()\n        )\n\n    # Workload function stubs\n\n    def txn_read(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT *\n                FROM orders\n                WHERE acc_no = %s\n                  AND id = %s\n                \"\"\",\n                random.choice(self.order_tuples),\n            ).fetchall()\n\n    def txn_new_order(self, conn: psycopg.Connection):\n\n        # generate a random account number to be used for\n        # for the order transaction\n        self.account_number = random.randint(0, 999)\n\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                INSERT INTO orders (acc_no, status, amount)\n                VALUES (%s, 'Pending', %s) RETURNING id\n                \"\"\",\n                (\n                    self.account_number,\n                    round(random.random() * 1000000, 2),\n                ),\n            )\n\n            # save the id that the server generated\n            self.id = cur.fetchone()[0]\n\n            # save the (acc_no, id) tuple to our deque list\n            # for future read transactions\n            self.order_tuples.append((self.account_number, self.id))\n\n    def txn_order_exec(self, conn: psycopg.Connection):\n\n        # with Psycopg, this is how you start an explicit transaction\n        with conn.transaction() as tx:\n            with conn.cursor() as cur:\n                cur.execute(\n                    \"\"\"\n                    SELECT *\n                    FROM ref_data\n                    WHERE acc_no = %s\n                    \"\"\",\n                    (\n                        self.account_number,\n                    ),\n                ).fetchall()\n\n                # simulate microservice doing something...\n                time.sleep(0.02)\n\n                cur.execute(\n                    \"\"\"\n                    UPDATE orders\n                    SET status = 'Complete'\n                    WHERE \n                        (acc_no, id) = (%s, %s)\n                    \"\"\",\n                    (\n                        self.account_number,\n                        self.id,\n                    ),\n                )\n</code></pre> <p>We are now finally ready to run our workload!</p>"},{"location":"getting_started/5/","title":"Check for runtime errors","text":"<p>As a test, we want to try to execute only 1 function at the time, once. We want to make sure we don't run into any runtime errors.</p> <p>Runtime errors can occur for various reasons, here are a few:</p> <ul> <li>your bind parameter type isn't compatible with the column datatype you try to insert into.   For example, you pass an <code>int</code> but the column is of type <code>VARCHAR</code>.</li> <li>The table you try to access does not exists.</li> </ul>"},{"location":"getting_started/5/#test-the-read-transaction","title":"Test the Read transaction","text":"<pre><code># we set read_pct to 100 to force just the read_txn\n# -i stands for iterations\n\n# PostgreSQL\ndbworkload run -w bank.py --uri 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable' \\\n  -i 1 --args '{\"read_pct\": 100}'\n\n# CockroachDB - notice we set the uri to point at the `bank` database           \u2b07\ufe0f\u2b07\ufe0f \ndbworkload run -w bank.py --uri 'postgres://cockroach:cockroach@localhost:26257/bank?sslmode=require' \\\n  -i 1 --args '{\"read_pct\": 100}'\n</code></pre> <p>The output is the same for any driver you choose, and it looks like below, stripped of some log messages for brevity.</p> <p>As in the <code>setup()</code> function we execute <code>select version();</code>, you will see in below output that this is the output from running <code>dbworkload</code> against PostgreSQL server.</p> <pre><code>2024-10-20 11:21:03,294 [INFO] (MainProcess MainThread) run:269: Starting workload Bank.20241020_152103\nMy thread ID is 0. The total count of threads is 1\nPostgreSQL 16.2 (Postgres.app) on aarch64-apple-darwin21.6.0, compiled by Apple clang version 14.0.0 (clang-1400.0.29.102), 64-bit\n2024-10-20 11:21:04,048 [INFO] (MainProcess MainThread) run:369: Requested iteration/duration limit reached\n2024-10-20 11:21:04,050 [INFO] (MainProcess MainThread) run:176: Printing final stats\n  elapsed  id           threads    tot_ops    tot_ops/s    period_ops    period_ops/s    mean(ms)    p50(ms)    p90(ms)    p95(ms)    p99(ms)    max(ms)\n---------  ---------  ---------  ---------  -----------  ------------  --------------  ----------  ---------  ---------  ---------  ---------  ---------\n        1  __cycle__          1          1            1             1               1        3.59       3.59       3.59       3.59       3.59       3.59\n        1  txn_read           1          1            1             1               1        3.58       3.58       3.58       3.58       3.58       3.58 \n\n2024-10-20 11:21:04,051 [INFO] (MainProcess MainThread) run:181: Printing summary for the full test run\n[...]\n</code></pre>"},{"location":"getting_started/5/#test-the-order-transaction","title":"Test the Order Transaction","text":"<p>We can repeat the  exercise to test the Order Transaction functions</p> <pre><code># now we set read_pct to zero\n\n# PostgreSQL\ndbworkload run -w bank.py --uri 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable' \\\n  -i 1 --args '{\"read_pct\": 0}'\n\n# CockroachDB \ndbworkload run -w bank.py -c 4 --uri 'postgres://root@localhost:26257/bank?sslmode=disable' \\\n  -i 1 --args '{\"read_pct\": 0}'\n</code></pre> <p>Here's the output from CockroachDB</p> <pre><code>2024-10-20 11:31:07,678 [INFO] (MainProcess MainThread) run:269: Starting workload Bank.20241020_153107\nMy thread ID is 0. The total count of threads is 1\nCockroachDB CCL v24.2.3 (x86_64-apple-darwin19, built 2024/09/23 22:30:57, go1.22.5 X:nocoverageredesign)\n2024-10-20 11:31:08,507 [INFO] (MainProcess MainThread) run:369: Requested iteration/duration limit reached\n2024-10-20 11:31:08,509 [INFO] (MainProcess MainThread) run:176: Printing final stats\n  elapsed  id                threads    tot_ops    tot_ops/s    period_ops    period_ops/s    mean(ms)    p50(ms)    p90(ms)    p95(ms)    p99(ms)    max(ms)\n---------  --------------  ---------  ---------  -----------  ------------  --------------  ----------  ---------  ---------  ---------  ---------  ---------\n        1  __cycle__               1          1            1             1               1       89.67      89.67      89.67      89.67      89.67      89.67\n        1  txn_new_order           1          1            1             1               1       21.20      21.20      21.20      21.20      21.20      21.20\n        1  txn_order_exec          1          1            1             1               1       68.45      68.45      68.45      68.45      68.45      68.45 \n\n2024-10-20 11:31:08,511 [INFO] (MainProcess MainThread) run:181: Printing summary for the full test run\n[...]\n</code></pre> <p>Success! The workload class runs without throwing any runtime errors \ud83d\ude80</p> <p>Important</p> <p>You should always, as a good practice, follow these 2 instructions:</p> <ul> <li>use <code>print()</code> throughout your workload class file to make sure you are generating, retrieving and querying the correct data. For example, print out your bind parameters, the result of <code>cur.fetchone()</code> and your variables like <code>self.order_tuples</code>, to make sure they contain the items you expect.</li> <li>inspect the mutated data on the SQL prompt matches with the intended workload goal.</li> </ul>"},{"location":"getting_started/6/","title":"Run the workload","text":"<p>We are finally ready to run our workload! In this section, we inspect a little bit more the output we saw in the previous section, explaining how to interpret it.</p>"},{"location":"getting_started/6/#do-a-test-run","title":"Do a test run","text":"<p>We now run a workload for 2 minutes (<code>-d 120</code>) and 4 connections (<code>-c 4</code>). We also want read operations to make 70% of the overall traffic.</p> <pre><code># targeting PostgreSQL Server\n$ dbworkload run -w bank.py \\\n  --uri 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable' \\\n  --args '{\"read_pct\": 70}' -d 120 -c 4\n2024-10-21 09:49:12,276 [INFO] (MainProcess MainThread) run:269: Starting workload Bank.20241021_134912\nMy thread ID is 0. The total count of threads is 4\nMy thread ID is 3. The total count of threads is 4\nMy thread ID is 1. The total count of threads is 4\nMy thread ID is 2. The total count of threads is 4\nPostgreSQL 16.2 (Postgres.app) on aarch64-apple-darwin21.6.0, compiled by Apple clang version 14.0.0 (clang-1400.0.29.102), 64-bit\nPostgreSQL 16.2 (Postgres.app) on aarch64-apple-darwin21.6.0, compiled by Apple clang version 14.0.0 (clang-1400.0.29.102), 64-bit\nPostgreSQL 16.2 (Postgres.app) on aarch64-apple-darwin21.6.0, compiled by Apple clang version 14.0.0 (clang-1400.0.29.102), 64-bit\nPostgreSQL 16.2 (Postgres.app) on aarch64-apple-darwin21.6.0, compiled by Apple clang version 14.0.0 (clang-1400.0.29.102), 64-bit\n  elapsed  id                threads    tot_ops    tot_ops/s    period_ops    period_ops/s    mean(ms)    p50(ms)    p90(ms)    p95(ms)    p99(ms)    max(ms)\n---------  --------------  ---------  ---------  -----------  ------------  --------------  ----------  ---------  ---------  ---------  ---------  ---------\n       10  __cycle__               4      4,792          479         4,792               0        7.22       0.27      24.55      25.46      26.57      72.60\n       10  txn_new_order           4      1,417          141         1,417               0        0.50       0.38       0.56       0.64       1.03      32.68\n       10  txn_order_exec          4      1,417          141         1,417               0       23.25      23.12      25.36      25.78      26.75      66.76\n       10  txn_read                4      3,375          337         3,375               0        0.27       0.23       0.36       0.41       0.52      31.26 \n\n  elapsed  id                threads    tot_ops    tot_ops/s    period_ops    period_ops/s    mean(ms)    p50(ms)    p90(ms)    p95(ms)    p99(ms)    max(ms)\n---------  --------------  ---------  ---------  -----------  ------------  --------------  ----------  ---------  ---------  ---------  ---------  ---------\n       20  __cycle__               4     10,492          524         5,700               0        6.95       0.26      24.27      25.18      26.26      40.11\n       20  txn_new_order           4      3,065          153         1,648               0        0.40       0.36       0.53       0.61       0.80       7.34\n       20  txn_order_exec          4      3,065          153         1,648               0       23.01      22.91      25.21      25.62      26.27      39.71\n       20  txn_read                4      7,427          371         4,052               0        0.24       0.22       0.35       0.40       0.51       5.02 \n\n[...truncating...]\n\n  elapsed  id                threads    tot_ops    tot_ops/s    period_ops    period_ops/s    mean(ms)    p50(ms)    p90(ms)    p95(ms)    p99(ms)    max(ms)\n---------  --------------  ---------  ---------  -----------  ------------  --------------  ----------  ---------  ---------  ---------  ---------  ---------\n      110  __cycle__               4     59,242          538         5,602               0        7.08       0.26      24.88      25.54      26.35      41.93\n      110  txn_new_order           4     17,685          160         1,617               0        0.42       0.37       0.55       0.62       0.92       9.19\n      110  txn_order_exec          4     17,685          160         1,617               0       23.48      23.70      25.45      25.75      26.43      41.59\n      110  txn_read                4     41,557          377         3,985               0        0.25       0.22       0.35       0.40       0.51      15.43 \n\n2024-10-21 09:51:12,330 [INFO] (MainProcess MainThread) run:369: Requested iteration/duration limit reached\n2024-10-21 09:51:12,331 [INFO] (MainProcess MainThread) run:176: Printing final stats\n  elapsed  id                threads    tot_ops    tot_ops/s    period_ops    period_ops/s    mean(ms)    p50(ms)    p90(ms)    p95(ms)    p99(ms)    max(ms)\n---------  --------------  ---------  ---------  -----------  ------------  --------------  ----------  ---------  ---------  ---------  ---------  ---------\n      120  __cycle__               4     64,652          538         5,410               0        7.39       0.27      24.87      25.57      26.39      55.03\n      120  txn_new_order           4     19,322          161         1,637               0        0.46       0.37       0.55       0.62       0.79      29.40\n      120  txn_order_exec          4     19,322          161         1,637               0       23.39      23.53      25.49      25.77      26.27      44.41\n      120  txn_read                4     45,330          377         3,773               0        0.25       0.23       0.35       0.40       0.49       5.49 \n\n2024-10-21 09:51:12,332 [INFO] (MainProcess MainThread) run:181: Printing summary for the full test run\n\n-------------  --------------------\nrun_name       Bank.20241021_134912\nstart_time     2024-10-21 13:49:12\nend_time       2024-10-21 13:51:12\ntest_duration  120\n-------------  --------------------\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   elapsed \u2502 id             \u2502   threads \u2502   tot_ops \u2502   tot_ops/s \u2502   mean(ms) \u2502   p50(ms) \u2502   p90(ms) \u2502   p95(ms) \u2502   p99(ms) \u2502   max(ms) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       120 \u2502 __cycle__      \u2502         4 \u2502    64,652 \u2502         538 \u2502       7.29 \u2502      0.30 \u2502     24.72 \u2502     25.51 \u2502     26.42 \u2502     98.36 \u2502\n\u2502       120 \u2502 txn_new_order  \u2502         4 \u2502    19,322 \u2502         161 \u2502       0.44 \u2502      0.37 \u2502      0.56 \u2502      0.64 \u2502      0.88 \u2502     33.19 \u2502\n\u2502       120 \u2502 txn_order_exec \u2502         4 \u2502    19,322 \u2502         161 \u2502      23.34 \u2502     23.34 \u2502     25.45 \u2502     25.78 \u2502     26.39 \u2502     97.89 \u2502\n\u2502       120 \u2502 txn_read       \u2502         4 \u2502    45,330 \u2502         377 \u2502       0.26 \u2502      0.23 \u2502      0.36 \u2502      0.41 \u2502      0.52 \u2502     31.26 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nParameter      Value\n-------------  -----------------------------------------------------------------------------------------------------------------------\nworkload_path  /path/to/workloads/bank.py\nconn_params    {'conninfo': 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable&amp;application_name=Bank', 'autocommit': True}\nconn_extras    {}\nconcurrency    4\nduration       120\niterations\nramp           0\nargs           {'read_pct': 70}\n</code></pre> <p>As we have created 4 connections, each connection will execute the <code>setup()</code> function and print its thread ID and server version.</p> <p><code>dbworkload</code> will then print out, every 10 seconds, a table with count and latency statistics for the functions it invoked.</p> <p>It is importand to understand that the statistics are on a function basis, not SQL transaction basis: as we saw while coding the <code>bank.py</code> workload class, certain functions do more than just executing a SQL statement. Function <code>txn_order_exec</code>, for example, executes 1 SELECT statements, then pauses for 20ms, and last it executes the UPDATE statement.</p> <p><code>dbworkload</code> thus shows you the latency to execute business functions rather than individual database statements/transactions.</p> <p>Finally, you have noticed there is a <code>__cycle__</code> entry in the table. This metric shows you the duration of the total execution of all functions returned in the <code>loop()</code>. This is helpful if the metric you use to benchmark your workload is neither at SQL transaction nor function level, but for the overall time taken to perform a series of functions.</p> <p>For example, you can measure the performance based on total execution time of an Order Transaction, that is, the sum of executing the <code>txn_new_order</code> and <code>txn_order_exec</code>.</p>"},{"location":"getting_started/6/#save-stats-to-csv","title":"Save stats to CSV","text":"<p><code>dbworkload</code> can save the statistics to a CSV file for later inspection.</p> <p>There are 2 common uses of the CSV files:</p> <ul> <li>as an input to plot charts.</li> <li>if <code>dbworkload</code> is executed from multiple servers towards the same target, you can merge the CSV files   to have a unified view of all the collected metrics.</li> </ul> <p>For this second run, we add flags for saving to CSV (<code>-s</code>) and for omitting printing to stdout (<code>-q</code>).</p> <pre><code>$ dbworkload run -w bank.py \\\n  --uri 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable' \\\n  --args '{\"read_pct\": 70}' -d 120 -c 4 \\\n  -s -q\n</code></pre> <p>After 2 minutes, check your directory for 2 new files:</p> <ul> <li>a <code>.csv</code> file</li> <li>a <code>.txt</code> file</li> </ul> <pre><code>$ ls -lh1 Bank.*\nBank.20241021_145825.csv\nBank.20241021_145825.txt\n</code></pre> <p>The <code>.txt</code> file shows a summary of the test run, whose name you've noticed takes the form of the workload name, <code>Bank</code>, followed by the timestamp.</p> <p>In the next session, we will use the <code>.csv</code> file to plot some charts.</p>"},{"location":"getting_started/7/","title":"Plot the metrics","text":"<p>We finished last section collectiong the metrics of a test run into a CSV file.</p> <p>In this section, we explore 2 convenient built-in functions.</p>"},{"location":"getting_started/7/#terminal-charts","title":"Terminal Charts","text":"<p>A quick and easy way to chart the metrics in the CSV file is to do so on the terminal client using the plot utility.</p> <p>Generally, you run <code>dbworkload</code> on a VM server which doesn't have a GUI, unlike the personal computer or MacBook you're reading this documentation from.</p> <p>On your Terminal client, run this command to chart the metrics</p> <pre><code>dbworkload util plot -i Bank.20241021_145825.csv\n</code></pre> <p>A screenshot renders the output best</p> <p></p>"},{"location":"getting_started/7/#interactive-html-charts","title":"Interactive HTML charts","text":"<p>Another charting option is using the more sophisticated util function <code>html</code>.</p> <pre><code>$ dbworkload util html -i Bank.20241021_145825.csv \n2024-10-21 11:26:30,304 [INFO] (MainProcess MainThread) util:423: Saved merged CSV file to '/path/to/workloads/Bank.20241021_145825.html'\n</code></pre> <p>Now open the <code>.html</code> file in your browser</p> <p></p> <p>You can scroll over the chart with your mouse to highlight datapoints. You can also zoom in to reveal more details.</p>"},{"location":"getting_started/7/#next","title":"Next","text":"<p>Congratulation, you reached the end of the tutorial! \ud83d\ude80\ud83d\ude80\ud83d\ude80</p> <p>Hopefully you grasped the basics of what <code>dbworkload</code> is and how to use it.</p> <p>Make sure you continue reading the other sections of these docs for more details about <code>dbworkload</code>'s inner workings.</p>"},{"location":"util/","title":"Utility functions","text":"<p>Here is the list of utility functions available</p> function description csv Generate CSV files from a YAML data generation file. gen_stub Generate a dbworkload class stub. html Save charts to HTML from the dbworkload statistics CSV file. merge Merge multiple sorted CSV files into 1+ files. merge_csvs Merge multiple dbworkload statistic CSV files. plot Plot terminal charts from the statistics CSV file. yaml Generate YAML data generation file from a DDL SQL file."},{"location":"util/csv/","title":"csv","text":""},{"location":"util/csv/#generating-csv-files","title":"Generating CSV files","text":"<p>You can seed a database quickly by letting <code>dbworkload</code> generate pseudo-random data and import it.</p> <p><code>dbworkload</code> takes the DDL as an input and creates an intermediate YAML file, with the definition of what data you want to create (a string, a number, a date, a bool..) based on the column data type.</p> <p>You then refine the YAML file to suit your needs, for example, the size of the string, a range for a date, the precision for a decimal, a choice among a discrete list of values..</p> <p>You can also specify what is the percentage of NULL for any column, or how many elements in an ARRAY type. You then specify the total row count, how many rows per file, and in what order, if any, to sort by.</p> <p>Then <code>dbworkload</code> will generate the data into CSV or TSV files, compress them if so requested.</p> <p>You can then optionally merge-sort the files.</p> <p>Consult file <code>workloads/postgres/bank.yaml</code> for a list of all available generators and options.</p>"},{"location":"util/csv/#see-also","title":"See also","text":"<ul> <li> <p>Generating intermediate data definition YAML file</p> </li> <li> <p>Blog: Generate multiple large sorted CSV files with pseudo-random data</p> </li> </ul>"},{"location":"util/gen_stub/","title":"gen_stub","text":""},{"location":"util/gen_stub/#generating-python-stub-files","title":"Generating Python stub files","text":"<p><code>dbworkload</code> requires a Python file that will be the foundation of the workload. In addition to setup and utility functions, this <code>.py</code> file will also require your SQL transactions in order to mirror the workload you want to re-create.</p> <p>Each SQL transaction will be contained within its own python function. We can speed up creation of these functions via the <code>gen_stub</code> command, which accepts a <code>.sql</code> file as an argument.  </p> <p>This page will only cover the stub functions generated, you can read more about the rest of the workload class here.</p>"},{"location":"util/gen_stub/#create-the-stub-file","title":"Create the stub file","text":"<p><code>gen_stub</code> accepts a <code>.sql</code> file as input and will create a <code>.py</code> as output.</p> <pre><code>dbworkload util gen_stub -i bank.sql\n</code></pre>"},{"location":"util/gen_stub/#options","title":"Options","text":"Option Usage <code>--input</code><code>-i</code> A <code>.sql</code> file containing one or more statements ending in a semi colon. Required: Yes  Default: None"},{"location":"util/gen_stub/#example","title":"Example","text":"<ol> <li> <p>Create a <code>.sql</code> file with various SQL statements. Let's call it <code>bank.sql</code>.</p> <pre><code>select * from t1 where id = 123;\n\nINSERT INTO t2 values (%s,%s);\n\nDELETE FROM \n    t3 where \n    id = %s returning *;\n\nSELECT now();\n</code></pre> </li> <li> <p>Run the <code>gen_stub</code> command, passing in <code>bank.sql</code> as the input.</p> <pre><code>dbworkload util gen_stub -i bank.sql\n</code></pre> </li> <li> <p>Open <code>bank.py</code> output and inspect function stubs. Notice one function per statement.</p> <pre><code>.\n.\nsetup() &amp; utility functions\n.\n.\n\n# Workload function stubs\n\n    def txn_0(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT *\n                FROM t1\n                WHERE id = 123\n                \"\"\",\n                (\n\n                ), \n            ).fetchall()\n\n    def txn_1(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                INSERT INTO t2\n                VALUES (%s,%s)\n                \"\"\",\n                (\n                    # add bind parameter, \n                    # add bind parameter, \n\n                ), \n            )\n\n    def txn_2(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                DELETE\n                FROM t3\n                WHERE id = %s RETURNING *\n                \"\"\",\n                (\n                    # add bind parameter, \n\n                ), \n            ).fetchall()\n\n    def txn_3(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT now()\n                \"\"\",\n                (\n\n                ), \n            ).fetchall()\n</code></pre> </li> <li> <p>View helpful tips at the bottom of the file.</p> <pre><code>'''\n# Quick random generators reminder\n\n# random string of 25 chars\nself.random_str(25),\n\n# random int between 0 and 100k\nrandom.randint(0, 100000),\n\n# random float with 2 decimals \nround(random.random()*1000000, 2)\n\n# now()\ndt.datetime.utcnow()\n\n# random timestamptz between certain dates,\n# expressed as unix ts\ndt.datetime.fromtimestamp(random.randint(1655032268, 1759232268))\n\n# random UUID\nuuid4()\n\n'''\n</code></pre> </li> <li> <p>Edit the workload</p> <p>Now that we have a stub and some hints, it's likely we'll need to make some edits and tune the workload to make it look more like what we want. Further reading on editing the workload can be found here: Edit the workload</p> </li> </ol>"},{"location":"util/gen_stub/#see-also","title":"See also","text":"<ul> <li>Create the workload class</li> <li>Edit the workload</li> </ul>"},{"location":"util/html/","title":"html","text":""},{"location":"util/merge/","title":"merge","text":""},{"location":"util/merge_csvs/","title":"merge_csvs","text":""},{"location":"util/plot/","title":"plot","text":""},{"location":"util/yaml/","title":"yaml","text":""},{"location":"util/yaml/#generate-yaml-data-generation-file-from-a-ddl-sql-file","title":"Generate YAML data generation file from a DDL SQL file","text":"<p><code>dbworkload</code> can assist with generating a stub of the data generation definition file, required by the <code>csv</code> command.</p>"},{"location":"util/yaml/#create-the-stub-file","title":"Create the stub file","text":"<p><code>yaml</code> accepts a file with DDL statements as input and will create a <code>.yaml</code> file as output.</p> <pre><code>dbworkload util yaml -i bank.ddl.sql\n</code></pre>"},{"location":"util/yaml/#options","title":"Options","text":"Option Usage <code>--input</code><code>-i</code> A SQL file containing one or more <code>CREATE TABLE</code> statements ending in a semi colon. Required: Yes  Default: None"},{"location":"util/yaml/#example","title":"Example","text":"<ol> <li> <p>Create a DDL file. Let's call it <code>bank.ddl</code>.</p> <pre><code>-- file: bank.ddl\n\nCREATE TABLE ref_data (\n    acc_no BIGINT PRIMARY KEY,\n    external_ref_id UUID,\n    created_time TIMESTAMPTZ,\n    acc_details VARCHAR\n);\n\nCREATE TABLE orders (\n    acc_no BIGINT NOT NULL,\n    id UUID NOT NULL default gen_random_uuid(),\n    status VARCHAR NOT NULL,\n    amount DECIMAL(15, 2),\n    ts TIMESTAMPTZ default now(),\n    CONSTRAINT pk PRIMARY KEY (acc_no, id)\n);\n</code></pre> </li> <li> <p>Run the <code>yaml</code> command, passing in <code>bank.ddl</code> as the input.</p> <pre><code>dbworkload util yaml -i bank.ddl\n</code></pre> </li> <li> <p>Open <code>bank.yaml</code> output and inspect YAML tree.     This is a configuration file with instructions on what type of data to generate.     Check section Types for a full list of available object types and arguments.</p> <pre><code>ref_data:\n  - count: 100\n    sort-by: []\n    columns:\n      acc_no:\n        type: integer\n        args:\n          min: -9223372036854775807\n          max: 9223372036854775807\n          seed: 64\n          null_pct: 0.3\n          array: 0\n      external_ref_id:\n        type: uuid\n        args:\n          seed: 76\n          null_pct: 0.25\n          array: 0\n      created_time:\n        type: timestamp\n        args:\n          start: \"2000-01-01\"\n          end: \"2024-12-31\"\n          format: \"%Y-%m-%d %H:%M:%S.%f\"\n          seed: 78\n          null_pct: 0.37\n          array: 0\n      acc_details:\n        type: string\n        args:\n          min: 10\n          max: 30\n          prefix: \"\"\n          seed: 54\n          null_pct: 0.38\n          array: 0\norders:\n  - count: 100\n    sort-by: []\n    columns:\n      acc_no:\n        type: integer\n        args:\n          min: -9223372036854775807\n          max: 9223372036854775807\n          seed: 83\n          null_pct: 0.0\n          array: 0\n    id:\n      type: uuid\n      args:\n        seed: 49\n        null_pct: 0.0\n        array: 0\n    status:\n      type: string\n      args:\n        min: 10\n        max: 30\n        prefix: \"\"\n        seed: 58\n        null_pct: 0.0\n        array: 0\n    amount:\n      type: float\n      args:\n        min: 0\n        max: 10000000000000\n        round: 2\n        seed: 11\n        null_pct: 0.33\n        array: 0\n    ts:\n      type: timestamp\n      args:\n        start: \"2000-01-01\"\n        end: \"2024-12-31\"\n        format: \"%Y-%m-%d %H:%M:%S.%f\"\n        seed: 100\n        null_pct: 0.26\n        array: 0\n</code></pre> </li> </ol>"},{"location":"util/yaml/#types","title":"Types","text":"<p>Below is a table with all available generators and their arguments</p> Generator Arguments Default constant value <code>str</code> \"simplefaker\" sequence start <code>int</code> 0 integer min <code>int</code> 1 max <code>int</code> 1,000,000,000 float min <code>int</code> 1 max <code>int</code> 1,000,000 round <code>int</code> 2 string min <code>int</code> 10 max <code>int</code> 50 prefix <code>str</code> &lt;blank&gt; json min <code>int</code> 10 max <code>int</code> 50 choice population <code>list[str]</code> [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"] weights  <code>list[int]</code> empty list cum_weights <code>list[int]</code> empty list timestamp start <code>str</code> \"2000-01-01\" end <code>str</code> \"2024-12-31\" format <code>str</code> \"%Y-%m-%d %H:%M:%S.%f\" date start <code>str</code> \"2000-01-01\" end <code>str</code> \"2024-12-31\" format <code>str</code> \"%Y-%m-%d\" time start <code>str</code> \"07:30:00\" end <code>str</code> \"22:30:00\" micros <code>bool</code> false uuid bool bit size <code>int</code> 10 bytes size <code>int</code> 10 <p>Furthermore, all but <code>constant</code> and <code>sequence</code> take these common arguments. <code>json</code> does not take <code>array</code>.</p> Arguments Default seed <code>float</code> 0 null_pct <code>float</code> 0 array <code>int</code> 0"},{"location":"util/yaml/#see-also","title":"See also","text":"<ul> <li>Seed the database tables</li> </ul>"}]}