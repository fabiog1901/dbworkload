{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dbworkload - DBMS workload utility","text":""},{"location":"#overview","title":"Overview","text":"<p>The goal of <code>dbworkload</code> is to ease the creation and execution of bespoke database workload scripts.</p> <p>The user is responsible for coding the workload logic as a Python <code>class</code>, while <code>dbworkload</code> takes care of providing ancillary features, such as configuring the workload concurrency, duration and/or iteration, and more.</p> <p>The user, by coding the class, has complete control of the workload flow: what transactions are executed in which order, and what statements are included in each transaction, plus what data to insert and how to generate it and manipulate it.</p>"},{"location":"#database-seeding","title":"Database seeding","text":"<p><code>dbworkload</code> can help with seeding a database by creating CSV files with random generated data, whose definition is supplied in a YAML file and can be extracted from a DDL SQL file.</p>"},{"location":"#how-it-works","title":"How it works","text":"<p>It\u2019s helpful to understand first what <code>dbworkload</code> does:</p> <ul> <li> <p>At runtime, <code>dbworkload</code> first imports the class you pass, <code>bank.py</code>.</p> </li> <li> <p>It spawns n threads for concurrent execution (see next section on Concurrency).</p> </li> <li>By default, it sets the connection to <code>autocommit</code> mode.</li> <li>Each thread creates a database connection - no need for a connection pool.</li> <li>In a loop, each <code>dbworkload</code> thread will:</li> <li>execute function <code>loop()</code> which returns a list of functions.</li> <li>execute each function in the list sequentially. Each function, typically, executes a SQL statement/transaction.</li> <li>Execution stats are funneled back to the MainThread, which aggregates and, optionally, prints them to stdout and saves them to a CSV file.</li> <li>If the connection drops, it will recreate it. You can also program how long you want the connection to last.</li> <li><code>dbworkload</code> stops once a limit has been reached (iteration/duration), or you Ctrl+C.</li> </ul>"},{"location":"#concurrency-processes-and-threads","title":"Concurrency - processes and threads","text":"<p><code>dbworkload</code> uses both the <code>multiprocessing</code> and <code>threading</code> library to achieve high concurrency, that is, opening multiple connections to the DBMS.</p> <p>There are 2 parameters that can be used to configure how many processes you want to create, and for each process, how many threads:</p> <ul> <li><code>--procs/-x</code>, to configure the count of processes (defaults to the CPU count)</li> <li><code>--concurrency/-c</code>, to configure the total number of connections (also referred to as executing threads)</li> </ul> <p><code>dbworkload</code> will spread the load across the processes, so that each process has an even amount of threads.</p> <p>Example: if we set <code>--procs 4</code> and <code>--concurrency 10</code>, dbworkload will create as follows:</p> <ul> <li>Process-1: MainThread + 1 extra thread. Total = 2</li> <li>Process-2: MainThread + 1 extra thread. Total = 2</li> <li>Process-3: MainThread + 2 extra threads. Total = 3</li> <li>Process-4: MainThread + 2 extra threads. Total = 3</li> </ul> <p>Total executing threads/connections = 10</p> <p>This allows you to fine tune the count of Python processes and threads to fit your system.</p> <p>Furthermore, each executing thread receives a unique ID (an integer). The ID is passed to the workload class with function <code>setup()</code>, along with the total count of threads, i.e. the value passed to <code>-c/--concurrency</code>. You can leverage the ID and the thread count in various ways, for example, to have each thread process a subset of a dataset.</p>"},{"location":"#generating-csv-files","title":"Generating CSV files","text":"<ul> <li>You can seed a database quickly by letting <code>dbworkload</code> generate pseudo-random data and import it.</li> <li><code>dbworkload</code> takes the DDL as an input and creates an intermediate YAML file, with the definition of what data you want to create (a string, a number, a date, a bool..) based on the column data type.</li> <li>You then refine the YAML file to suit your needs, for example, the size of the string, a range for a date, the precision for a decimal, a choice among a discrete list of values..</li> <li>You can also specify what is the percentage of NULL for any column, or how many elements in an ARRAY type.</li> <li>You then specify the total row count, how many rows per file, and in what order, if any, to sort by.</li> <li>Then <code>dbworkload</code> will generate the data into CSV or TSV files, compress them if so requested.</li> <li>You can then optionally merge-sort the files using command <code>merge</code>.</li> </ul> <p>Write up blog: Generate multiple large sorted csv files with pseudo-random data</p> <p>Find out more on the <code>yaml</code>, <code>csv</code> and <code>merge</code> commands by running</p> <pre><code>dbworkload util --help\n</code></pre> <p>Consult file <code>workloads/postgres/bank.yaml</code> for a list of all available generators and options.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#acknowledgments","title":"Acknowledgments","text":"<p>Some methods and classes have been taken and modified from, or inspired by, https://github.com/cockroachdb/movr</p>"},{"location":"cli/","title":"CLI","text":"<p>This page provides documentation for the command line commands and their options.</p>"},{"location":"cli/#dbworkload","title":"dbworkload","text":"<p>dbworkload v0.5.1: DBMS workload utility.</p> <p>Usage:</p> <pre><code>dbworkload [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -v, --version         Print the version and exit\n  --install-completion  Install completion for the current shell.\n  --show-completion     Show completion for the current shell, to copy it or\n                        customize the installation.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli/#run","title":"run","text":"<p>Run the workload.</p> <p>Usage:</p> <pre><code>dbworkload run [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -w, --workload FILE             Filepath to the workload module.\n  --driver [postgres|mysql|maria|oracle|sqlserver|mongo|cassandra]\n                                  DBMS driver.\n  --uri TEXT                      The connection URI to the database.\n  -x, --procs INTEGER             Number of processes to spawn. Defaults to\n                                  &lt;system-cpu-count&gt;.\n  --args TEXT                     JSON string, or filepath to a JSON/YAML\n                                  file, to pass to Workload.\n  -c, --concurrency INTEGER       Number of concurrent workers.  [default: 1]\n  -r, --ramp INTEGER              Ramp up time in seconds.  [default: 0]\n  -i, --iterations INTEGER        Total number of iterations. Defaults to &lt;ad\n                                  infinitum&gt;.\n  -d, --duration INTEGER          Duration in seconds. Defaults to &lt;ad\n                                  infinitum&gt;.\n  -k, --conn-duration INTEGER     The number of seconds to keep database\n                                  connection alive before restarting. Defaults\n                                  to &lt;ad infinitum&gt;.\n  -a, --app-name TEXT             The application name specified by the\n                                  client. Defaults to &lt;db-name&gt;.\n  --no-autocommit                 Unset autocommit in the connections.\n  -p, --port INTEGER              The port of the Prometheus server.\n                                  [default: 26260]\n  -q, --quiet                     Disable printing intermediate stats.\n  -s, --save                      Save stats to CSV files.\n  -l, --log-level [debug|info|warning|error]\n                                  Set the logging level.  [default: info]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#util","title":"util","text":"<p>Various utils.</p> <p>Usage:</p> <pre><code>dbworkload util [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#csv","title":"csv","text":"<p>Generate CSV files from a YAML data generation file.</p> <p>Usage:</p> <pre><code>dbworkload util csv [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE                Filepath to the YAML data generation file.\n                                  [required]\n  -o, --output DIRECTORY          Output directory for the CSV files. Defaults\n                                  to &lt;input-basename&gt;.\n  -x, --procs INTEGER             Number of processes to spawn. Defaults to\n                                  &lt;system-cpu-count&gt;.\n  --csv-max-rows INTEGER          Max count of rows per resulting CSV file.\n                                  [default: 100000]\n  -n, --hostname TEXT             The hostname of the http server that serves\n                                  the CSV files.\n  -p, --port INTEGER              The port of the http server that servers the\n                                  CSV files.  [default: 3000]\n  -c, --compression [bz2|gzip|xz|zip]\n                                  The compression format.\n  -d, --delimiter TEXT            The delimeter char to use for the CSV files.\n                                  Defaults to \"tab\".\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#gen_stub","title":"gen_stub","text":"<p>Generate a dbworkload class stub.</p> <p>Usage:</p> <pre><code>dbworkload util gen_stub [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE  Input SQL file  [required]\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli/#html","title":"html","text":"<p>Save charts to HTML from the dbworkload statistics CSV file.</p> <p>Usage:</p> <pre><code>dbworkload util html [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE  Input CSV file  [required]\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli/#merge","title":"merge","text":"<p>Merge multiple sorted CSV files into 1+ files.</p> <p>Usage:</p> <pre><code>dbworkload util merge [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input DIRECTORY   Directory of files to be merged  [required]\n  -o, --output PATH       Output filepath. Defaults to &lt;input&gt;.merged.\n  --csv-max-rows INTEGER  Max count of rows per resulting CSV file.  [default:\n                          100000]\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"cli/#merge_csvs","title":"merge_csvs","text":"<p>Merge multiple dbworkload statistic CSV files.</p> <p>Usage:</p> <pre><code>dbworkload util merge_csvs [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input_dir DIRECTORY  Input CSV directory  [required]\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli/#plot","title":"plot","text":"<p>Plot charts from the dbworkload statistics CSV file.</p> <p>Usage:</p> <pre><code>dbworkload util plot [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE  Input CSV file  [required]\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli/#yaml","title":"yaml","text":"<p>Generate YAML data generation file from a DDL SQL file.</p> <p>Usage:</p> <pre><code>dbworkload util yaml [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE   Filepath to the DDL SQL file.  [required]\n  -o, --output PATH  Output filepath. Defaults to &lt;input-basename&gt;.yaml.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"drivers/","title":"Drivers","text":""},{"location":"drivers/#postgres","title":"postgres","text":"<p>For technologies such as PostgreSQL, CockroachDB</p> <p>Driver documentation: Psycopg 3.</p> <pre><code># installation\npip install dbworkload[postgres]\n\n# sample use\ndbworkload run -w workloads/postgres/bank.py \\\n  --uri 'postgres://cockroach:cockroach@localhost:26257/bank?sslmode=require' \\\n  -l debug --args '{\"read_pct\":50}' -i 1 -c 1\n</code></pre>"},{"location":"drivers/#mysql","title":"mysql","text":"<p>For technologies such as MySQL, TiDB, Singlestore</p> <p>Driver documentation: MySQL Connector/Python Developer Guide.</p> <pre><code># installation\npip3 install dbworkload[mysql]\n\n# sample use\ndbworkload run -w workloads/mysql/bank.py \\\n  --uri 'user=root,password=London123,host=localhost,port=3306,database=bank,client_flags=SSL' \\\n   --driver mysql --args '{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#mariadb","title":"mariadb","text":"<p>Driver documentation: MariaDB Connector/Python.</p> <pre><code># installation\npip3 install dbworkload[mariadb]\n\n# sample use\ndbworkload run -w workloads/mariadb/bank.py \\\n  --uri 'user=user1,password=password1,host=localhost,port=3306,database=bank' \\\n  --driver maria --args '{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#oracle","title":"oracle","text":"<p>Driver documentation: python-oracledb\u2019s documentation.</p> <pre><code># installation\npip3 install dbworkload[oracle]\n\n# sample use\ndbworkload run -w workloads/oracle/bank.py \\\n  --uri \"user=admin,password=password1,dsn=\"myhost.host2.us-east-1.rds.amazonaws.com:1521/OMS\" \\\n  --driver oracle --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#sqlserver","title":"sqlserver","text":"<p>Under construction...</p> <p>Driver documentation: Python SQL driver.</p> <pre><code># installation\npip3 install dbworkload[sqlserver]\n\n# sample use\ndbworkload run -w workloads/sqlserver/bank.py \\\n  --uri \"\" \\\n   --driver sqlserver --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#mongo","title":"mongo","text":"<p>Driver documentation: MongoDB PyMongo Documentation.</p> <pre><code># installation\npip3 install dbworkload[mongo]\n\n# sample use\ndbworkload run -w workloads/mongo/bank.py \\\n  --uri \"mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000\" \\\n  --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#scylla-driver","title":"scylla-driver","text":"<p>For technologies such as Cassandra, ScyllaDB</p> <p>Under construction...</p> <p>Driver documentation: Python Driver for Scylla and Apache Cassandra.</p> <pre><code># installation\npip3 install dbworkload[cassandra]\n\n# sample use\ndbworkload run -w workloads/cassandra/bank.py \\\n  --uri \"\" \\\n   --driver cassandra --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"how_it_works/","title":"How it works","text":"<p>It\u2019s helpful to understand first what <code>dbworkload</code> does:</p> <ul> <li> <p>At runtime, <code>dbworkload</code> first imports the class you pass, <code>bank.py</code>.</p> </li> <li> <p>It spawns n threads for concurrent execution (see next section on Concurrency).</p> </li> <li>By default, it sets the connection to <code>autocommit</code> mode.</li> <li>Each thread creates a database connection - no need for a connection pool.</li> <li>In a loop, each <code>dbworkload</code> thread will:</li> <li>execute function <code>loop()</code> which returns a list of functions.</li> <li>execute each function in the list sequentially. Each function, typically, executes a SQL statement/transaction.</li> <li>Execution stats are funneled back to the MainThread, which aggregates and, optionally, prints them to stdout and saves them to a CSV file.</li> <li>If the connection drops, it will recreate it. You can also program how long you want the connection to last.</li> <li><code>dbworkload</code> stops once a limit has been reached (iteration/duration), or you Ctrl+C.</li> </ul>"},{"location":"how_it_works/#concurrency-processes-and-threads","title":"Concurrency - processes and threads","text":"<p><code>dbworkload</code> uses both the <code>multiprocessing</code> and <code>threading</code> library to achieve high concurrency, that is, opening multiple connections to the DBMS.</p> <p>There are 2 parameters that can be used to configure how many processes you want to create, and for each process, how many threads:</p> <ul> <li><code>--procs/-x</code>, to configure the count of processes (defaults to the CPU count)</li> <li><code>--concurrency/-c</code>, to configure the total number of connections (also referred to as executing threads)</li> </ul> <p><code>dbworkload</code> will spread the load across the processes, so that each process has an even amount of threads.</p> <p>Example: if we set <code>--procs 4</code> and <code>--concurrency 10</code>, dbworkload will create as follows:</p> <ul> <li>Process-1: MainThread + 1 extra thread. Total = 2</li> <li>Process-2: MainThread + 1 extra thread. Total = 2</li> <li>Process-3: MainThread + 2 extra threads. Total = 3</li> <li>Process-4: MainThread + 2 extra threads. Total = 3</li> </ul> <p>Total executing threads/connections = 10</p> <p>This allows you to fine tune the count of Python processes and threads to fit your system.</p> <p>Furthermore, each executing thread receives a unique ID (an integer). The ID is passed to the workload class with function <code>setup()</code>, along with the total count of threads, i.e. the value passed to <code>-c/--concurrency</code>. You can leverage the ID and the thread count in various ways, for example, to have each thread process a subset of a dataset.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#software-requirements","title":"Software Requirements","text":"<p><code>dbworkload</code> requires at least Python 3.8 and the <code>pip</code> utility, installed and upgraded.</p> <p><code>dbworkload</code> dependencies are installed automatically by the <code>pip</code> tool.</p> <p>It has run successfully on Ubuntu 20.04+, MacOSX on both Intel and Apple silicone.</p>"},{"location":"installation/#dbworkload-installation","title":"dbworkload installation","text":"<p><code>dbworkload</code> comes already pre-packaged, available from PyPI.</p> <p>Generally, you want to specify which of the supported drivers you want to install.</p> <p>In below example, we install with the Psycopg3 driver, so we run</p> <pre><code>pip3 install dbworkload[postgres]\n</code></pre> <p>Confirm installation is successful by running</p> <pre><code>$ dbworkload --version\ndbworkload : 0.5.1\nPython     : 3.11.3\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Example using PostgreSQL Server and CockroachDB.</p> <p>Class <code>Bank</code> in file <code>workloads/postgres/bank.py</code> is an example of one such user-created workload. The class defines 3 simple transactions that have to be executed by <code>dbworkload</code>. Have a look at the <code>bank.py</code>, <code>bank.yaml</code> and <code>bank.sql</code> in the <code>workload/postgres/</code> folder in this project.</p> <p>Head to file <code>workload/postgres/bank.sql</code> to see what the database schema look like. We have 2 tables:</p> <ul> <li>the <code>transactions</code> table, where we record the bank payment transactions.</li> <li>the <code>ref_data</code> table.</li> </ul> <p>Take a close look at this last table: each column represent a different type, which brings us to the next file.</p> <p>File <code>bank.yaml</code> is the data generation definition file. For each column of table <code>ref_data</code>, we deterministically generate random data. This file is meant as a guide to show what type of data can be generated, and what args are required.</p> <p>File <code>bank.py</code> defines the workload. The workload is defined as a class object. The class defines 2 methods: <code>loop()</code> and the constructor, <code>__init__()</code>. All other methods are part of the application logic of the workload. Read the comments along the code for more information.</p> <p>Let's run the sample Bank workload.</p>"},{"location":"getting_started/0/","title":"Env Setup","text":"<pre><code># using ubuntu 20.04 LTS\nsudo apt update\nsudo apt install -y python3-pip\n\n# upgrade pip - must have pip version 20.3+ \npip3 install --upgrade pip\n\npip3 install dbworkload[postgres]\n\nmkdir workloads\ncd workloads\n\n# the workload class\nwget https://raw.githubusercontent.com/fabiog1901/dbworkload/main/workloads/postgres/bank.py\n\n# the DDL file\nwget https://raw.githubusercontent.com/fabiog1901/dbworkload/main/workloads/postgres/bank.sql\n\n# the data generation definition file\nwget https://raw.githubusercontent.com/fabiog1901/dbworkload/main/workloads/postgres/bank.yaml\n</code></pre>"},{"location":"getting_started/1/","title":"Init the workload","text":"<p>Make sure your CockroachDB cluster or PostgreSQL server is up and running. Please note: this guide assumes the database server and dbworkload are both runnining locally and can communicate with each other via <code>localhost</code>.</p> <p>Connect to the SQL prompt and execute the DDL statements in the <code>bank.sql</code> file. In CockroachDB, you can simply run</p> <pre><code>sql&gt; \\i bank.sql\n</code></pre> <p>Next, generate some CSV data to seed the database:</p> <pre><code>dbworkload util csv -i bank.yaml -x 1\n</code></pre> <p>The CSV files will be located inside a <code>bank</code> directory.</p> <pre><code>$ ls -lh bank\ntotal 1032\n-rw-r--r--  1 fabio  staff   513K Apr  9 13:01 ref_data.0_0_0.csv\n\n$ head -n2 bank/ref_data.0_0_0.csv \n0       simplefaker     b66ab5dc-1fcc-4ac8-8ad0-70bbbb463f00    alpha   16381   {124216.6,416559.9,355271.42,443666.45,689859.03,461510.94,31766.46,727918.45,361202.5,561364.1}        12421672576.9632        2022-10-18 04:57:37.613512      2022-10-18     13:36:48 1001010011      \\xe38a2e10b400a8e77eda  {ID-cUJeNcMZ,ID-mWxhyiqN,ID-0FnlVOO5}   0       \"{\"\"k\"\":\"\"cUJNcMZ\"\"}\"\n1       simplefaker     f2ebb78a-5af3-4755-8c22-2ad06aa3b26c    bravo   39080           35527177861.6551        2022-12-25 09:12:04.771673      2022-12-25      13:05:42        0110111101      \\x5a2efedf253aa3fbeea8  {ID-gQkRkMxIkSjihWcWTcr,ID-o7iDzl9AMJoFfduo6Hz,ID-5BS3MlZgOjxFZRBgBmf}  0       \"{\"\"k\"\":\"\"5Di0UHLWMEuR7\"\"}\"\n</code></pre> <p>Now you can import the CSV file. In CockroachDB, my favorite method is to use a webserver to serve the CSV file. Open a new terminal then start a simple python server</p> <pre><code>cd workloads\ncd bank\npython3 -m http.server 3000\n</code></pre> <p>If you open your browser at http://localhost:3000 you should see file <code>ref_data.0_0_0.csv</code> being served.</p> <p>At the SQL prompt, import the file</p> <pre><code>sql&gt; IMPORT INTO ref_data CSV DATA ('http://localhost:3000/ref_data.0_0_0.csv') WITH delimiter = e'\\t'; \n</code></pre> <p>In PostgreSQL Server, at the SQL prompt, just use <code>COPY</code></p> <pre><code>bank=# COPY ref_data FROM '/Users/fabio/workloads/bank/ref_data.0_0_0.csv' WITH CSV DELIMITER AS e'\\t';\nCOPY 100\nTime: 2.713 ms\n</code></pre>"},{"location":"getting_started/2/","title":"Run the workload","text":"<p>Run the workload using 4 connections for 120 seconds or 100k cycles, whichever comes first.</p> <pre><code># CockroachDB\ndbworkload run -w bank.py -c 4 --uri 'postgres://root@localhost:26257/bank?sslmode=disable' -d 120 -i 100000\n\n# PostgreSQL\ndbworkload run -w bank.py -c 4 --uri 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable' -d 120 -i 100000\n</code></pre> <p><code>dbworkload</code> will output rolling statistics about throughput and latency for each transaction in your workload class</p> <pre><code>  elapsed  id               threads    tot_ops    tot_ops/s    period_ops    period_ops/s    mean(ms)    p50(ms)    p90(ms)    p95(ms)    p99(ms)    max(ms)\n---------  -------------  ---------  ---------  -----------  ------------  --------------  ----------  ---------  ---------  ---------  ---------  ---------\n        8  __cycle__              4        190        23.00           190           19.00      107.20      53.90     200.14     200.71     202.06     204.84\n        8  read                   4         97        12.00            97            9.00       25.59      18.81      51.72      52.71      68.33      81.11\n        8  txn1_new               4         93        11.00            93            9.00       46.17      46.93      48.44      49.08      61.33      67.98\n        8  txn2_verify            4         93        11.00            93            9.00       76.12      83.02      84.46      84.56      95.24     102.75\n        8  txn3_finalize          4         93        11.00            93            9.00       69.99      69.03      80.65      83.21      93.16      99.95 \n[...]\n</code></pre> <p>You can always use pgAdmin for PostgreSQL Server or the DB Console for CockroachDB to view your workload, too.</p> <p>There are many built-in options. Check them out with</p> <pre><code>dbworkload --help\n</code></pre>"},{"location":"util/","title":"Utility functions","text":"function description csv Generate CSV files from a YAML data generation file. gen_stub Generate a dbworkload class stub. html Save charts to HTML from the dbworkload statistics CSV file. merge Merge multiple sorted CSV files into 1+ files. merge_csvs Merge multiple dbworkload statistic CSV files. plot Plot charts from the dbworkload statistics CSV file. yaml Generate YAML data generation file from a DDL SQL file."},{"location":"util/csv/","title":"csv","text":"<ul> <li>You can seed a database quickly by letting <code>dbworkload</code> generate pseudo-random data and import it.</li> <li><code>dbworkload</code> takes the DDL as an input and creates an intermediate YAML file, with the definition of what data you want to create (a string, a number, a date, a bool..) based on the column data type.</li> <li>You then refine the YAML file to suit your needs, for example, the size of the string, a range for a date, the precision for a decimal, a choice among a discrete list of values..</li> <li>You can also specify what is the percentage of NULL for any column, or how many elements in an ARRAY type.</li> <li>You then specify the total row count, how many rows per file, and in what order, if any, to sort by.</li> <li>Then <code>dbworkload</code> will generate the data into CSV or TSV files, compress them if so requested.</li> <li>You can then optionally merge-sort the files using command <code>merge</code>.</li> </ul> <p>Write up blog: Generate multiple large sorted csv files with pseudo-random data</p> <p>Consult file <code>workloads/postgres/bank.yaml</code> for a list of all available generators and options.</p>"},{"location":"util/gen_stub/","title":"gen_stub","text":""},{"location":"util/html/","title":"html","text":""},{"location":"util/merge/","title":"merge","text":""},{"location":"util/merge_csvs/","title":"merge_csvs","text":""},{"location":"util/plot/","title":"plot","text":""},{"location":"util/yaml/","title":"yaml","text":""}]}