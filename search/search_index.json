{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dbworkload - DBMS workload utility","text":""},{"location":"#overview","title":"Overview","text":"<p>The goal of <code>dbworkload</code> is to ease the creation and execution of bespoke database workload scripts.</p> <p>The user is responsible for coding the workload logic as a Python <code>class</code>, while <code>dbworkload</code> takes care of providing ancillary features, such as configuring the workload concurrency, duration and/or iteration, and more.</p> <p>The user, by coding the class, has complete control of the workload flow: what transactions are executed in which order, and what statements are included in each transaction, plus what data to insert and how to generate it and manipulate it.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#acknowledgments","title":"Acknowledgments","text":"<p>Some methods and classes have been taken and modified from, or inspired by, https://github.com/cockroachdb/movr</p>"},{"location":"cli/","title":"CLI","text":"<p>This page provides documentation for the command line arguments and options.</p>"},{"location":"cli/#dbworkload","title":"dbworkload","text":"<p>dbworkload v0.5.1: DBMS workload utility.</p> <p>Usage:</p> <pre><code>dbworkload [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -v, --version         Print the version and exit\n  --install-completion  Install completion for the current shell.\n  --show-completion     Show completion for the current shell, to copy it or\n                        customize the installation.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli/#run","title":"run","text":"<p>Run the workload.</p> <p>Usage:</p> <pre><code>dbworkload run [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -w, --workload FILE             Filepath to the workload module.\n  --driver [postgres|mysql|maria|oracle|sqlserver|mongo|cassandra]\n                                  DBMS driver.\n  --uri TEXT                      The connection URI to the database.\n  -x, --procs INTEGER             Number of processes to spawn. Defaults to\n                                  &lt;system-cpu-count&gt;.\n  --args TEXT                     JSON string, or filepath to a JSON/YAML\n                                  file, to pass to Workload.\n  -c, --concurrency INTEGER       Number of concurrent workers.  [default: 1]\n  -r, --ramp INTEGER              Ramp up time in seconds.  [default: 0]\n  -i, --iterations INTEGER        Total number of iterations. Defaults to &lt;ad\n                                  infinitum&gt;.\n  -d, --duration INTEGER          Duration in seconds. Defaults to &lt;ad\n                                  infinitum&gt;.\n  -k, --conn-duration INTEGER     The number of seconds to keep database\n                                  connection alive before restarting. Defaults\n                                  to &lt;ad infinitum&gt;.\n  -a, --app-name TEXT             The application name specified by the\n                                  client. Defaults to &lt;db-name&gt;.\n  --no-autocommit                 Unset autocommit in the connections.\n  -p, --port INTEGER              The port of the Prometheus server.\n                                  [default: 26260]\n  -q, --quiet                     Disable printing intermediate stats.\n  -s, --save                      Save stats to CSV files.\n  -l, --log-level [debug|info|warning|error]\n                                  Set the logging level.  [default: info]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#util","title":"util","text":"<p>Various utils.</p> <p>Usage:</p> <pre><code>dbworkload util [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#csv","title":"csv","text":"<p>Generate CSV files from a YAML data generation file.</p> <p>Usage:</p> <pre><code>dbworkload util csv [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE                Filepath to the YAML data generation file.\n                                  [required]\n  -o, --output DIRECTORY          Output directory for the CSV files. Defaults\n                                  to &lt;input-basename&gt;.\n  -x, --procs INTEGER             Number of processes to spawn. Defaults to\n                                  &lt;system-cpu-count&gt;.\n  --csv-max-rows INTEGER          Max count of rows per resulting CSV file.\n                                  [default: 100000]\n  -n, --hostname TEXT             The hostname of the http server that serves\n                                  the CSV files.\n  -p, --port INTEGER              The port of the http server that servers the\n                                  CSV files.  [default: 3000]\n  -c, --compression [bz2|gzip|xz|zip]\n                                  The compression format.\n  -d, --delimiter TEXT            The delimeter char to use for the CSV files.\n                                  Defaults to \"tab\".\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#gen_stub","title":"gen_stub","text":"<p>Generate a dbworkload class stub.</p> <p>Usage:</p> <pre><code>dbworkload util gen_stub [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE  Input SQL file  [required]\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli/#html","title":"html","text":"<p>Save charts to HTML from the dbworkload statistics CSV file.</p> <p>Usage:</p> <pre><code>dbworkload util html [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE  Input CSV file  [required]\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli/#merge","title":"merge","text":"<p>Merge multiple sorted CSV files into 1+ files.</p> <p>Usage:</p> <pre><code>dbworkload util merge [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input DIRECTORY   Directory of files to be merged  [required]\n  -o, --output PATH       Output filepath. Defaults to &lt;input&gt;.merged.\n  --csv-max-rows INTEGER  Max count of rows per resulting CSV file.  [default:\n                          100000]\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"cli/#merge_csvs","title":"merge_csvs","text":"<p>Merge multiple dbworkload statistic CSV files.</p> <p>Usage:</p> <pre><code>dbworkload util merge_csvs [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input_dir DIRECTORY  Input CSV directory  [required]\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli/#plot","title":"plot","text":"<p>Plot charts from the dbworkload statistics CSV file.</p> <p>Usage:</p> <pre><code>dbworkload util plot [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE  Input CSV file  [required]\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli/#yaml","title":"yaml","text":"<p>Generate YAML data generation file from a DDL SQL file.</p> <p>Usage:</p> <pre><code>dbworkload util yaml [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -i, --input FILE   Filepath to the DDL SQL file.  [required]\n  -o, --output PATH  Output filepath. Defaults to &lt;input-basename&gt;.yaml.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"drivers/","title":"Drivers","text":""},{"location":"drivers/#postgres","title":"postgres","text":"<p>For technologies such as PostgreSQL, CockroachDB</p> <p>Driver documentation: Psycopg 3.</p> <pre><code># installation\npip install dbworkload[postgres]\n\n# sample use\ndbworkload run -w workloads/postgres/bank.py \\\n  --uri 'postgres://cockroach:cockroach@localhost:26257/bank?sslmode=require' \\\n  -l debug --args '{\"read_pct\":50}' -i 1 -c 1\n</code></pre>"},{"location":"drivers/#mysql","title":"mysql","text":"<p>For technologies such as MySQL, TiDB, Singlestore</p> <p>Driver documentation: MySQL Connector/Python Developer Guide.</p> <pre><code># installation\npip3 install dbworkload[mysql]\n\n# sample use\ndbworkload run -w workloads/mysql/bank.py \\\n  --uri 'user=root,password=London123,host=localhost,port=3306,database=bank,client_flags=SSL' \\\n   --driver mysql --args '{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#mariadb","title":"mariadb","text":"<p>Driver documentation: MariaDB Connector/Python.</p> <pre><code># installation\npip3 install dbworkload[mariadb]\n\n# sample use\ndbworkload run -w workloads/mariadb/bank.py \\\n  --uri 'user=user1,password=password1,host=localhost,port=3306,database=bank' \\\n  --driver maria --args '{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#oracle","title":"oracle","text":"<p>Driver documentation: python-oracledb\u2019s documentation.</p> <pre><code># installation\npip3 install dbworkload[oracle]\n\n# sample use\ndbworkload run -w workloads/oracle/bank.py \\\n  --uri \"user=admin,password=password1,dsn=\"myhost.host2.us-east-1.rds.amazonaws.com:1521/OMS\" \\\n  --driver oracle --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#sqlserver","title":"sqlserver","text":"<p>Under construction...</p> <p>Driver documentation: Python SQL driver.</p> <pre><code># installation\npip3 install dbworkload[sqlserver]\n\n# sample use\ndbworkload run -w workloads/sqlserver/bank.py \\\n  --uri \"\" \\\n   --driver sqlserver --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#mongo","title":"mongo","text":"<p>Driver documentation: MongoDB PyMongo Documentation.</p> <pre><code># installation\npip3 install dbworkload[mongo]\n\n# sample use\ndbworkload run -w workloads/mongo/bank.py \\\n  --uri \"mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000\" \\\n  --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"drivers/#scylla-driver","title":"scylla-driver","text":"<p>For technologies such as Cassandra, ScyllaDB</p> <p>Under construction...</p> <p>Driver documentation: Python Driver for Scylla and Apache Cassandra.</p> <pre><code># installation\npip3 install dbworkload[cassandra]\n\n# sample use\ndbworkload run -w workloads/cassandra/bank.py \\\n  --uri \"\" \\\n   --driver cassandra --args='{\"read_pct\":50}' -i 10\n</code></pre>"},{"location":"how_it_works/","title":"How it works","text":"<p>It\u2019s helpful to understand first what <code>dbworkload</code> does:</p> <ul> <li> <p>At runtime, <code>dbworkload</code> first imports the class you pass, <code>bank.py</code>.</p> </li> <li> <p>It spawns n threads for concurrent execution (see next section on Concurrency).</p> </li> <li>By default, it sets the connection to <code>autocommit</code> mode.</li> <li>Each thread creates a database connection - no need for a connection pool.</li> <li>In a loop, each <code>dbworkload</code> thread will:</li> <li>execute function <code>loop()</code> which returns a list of functions.</li> <li>execute each function in the list sequentially. Each function, typically, executes a SQL statement/transaction.</li> <li>Execution stats are funneled back to the MainThread, which aggregates and, optionally, prints them to stdout and saves them to a CSV file.</li> <li>If the connection drops, it will recreate it. You can also program how long you want the connection to last.</li> <li><code>dbworkload</code> stops once a limit has been reached (iteration/duration), or you Ctrl+C.</li> </ul>"},{"location":"how_it_works/#concurrency-processes-and-threads","title":"Concurrency - processes and threads","text":"<p><code>dbworkload</code> uses both the <code>multiprocessing</code> and <code>threading</code> library to achieve high concurrency, that is, opening multiple connections to the DBMS.</p> <p>There are 2 parameters that can be used to configure how many processes you want to create, and for each process, how many threads:</p> <ul> <li><code>--procs/-x</code>, to configure the count of processes (defaults to the CPU count)</li> <li><code>--concurrency/-c</code>, to configure the total number of connections (also referred to as executing threads)</li> </ul> <p><code>dbworkload</code> will spread the load across the processes, so that each process has an even amount of threads.</p> <p>Example: if we set <code>--procs 4</code> and <code>--concurrency 10</code>, dbworkload will create as follows:</p> <ul> <li>Process-1: MainThread + 1 extra thread. Total = 2</li> <li>Process-2: MainThread + 1 extra thread. Total = 2</li> <li>Process-3: MainThread + 2 extra threads. Total = 3</li> <li>Process-4: MainThread + 2 extra threads. Total = 3</li> </ul> <p>Total executing threads/connections = 10</p> <p>This allows you to fine tune the count of Python processes and threads to fit your system.</p> <p>Furthermore, each executing thread receives a unique ID (an integer). The ID is passed to the workload class with function <code>setup()</code>, along with the total count of threads, i.e. the value passed to <code>-c/--concurrency</code>. You can leverage the ID and the thread count in various ways, for example, to have each thread process a subset of a dataset.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#software-requirements","title":"Software Requirements","text":"<p><code>dbworkload</code> requires at least Python 3.8 and the <code>pip</code> utility, installed and upgraded.</p> <p><code>dbworkload</code> dependencies are installed automatically by the <code>pip</code> tool.</p> <p>It has run successfully on Ubuntu 20.04+, MacOSX on both Intel and Apple silicone.</p>"},{"location":"installation/#dbworkload-installation","title":"dbworkload installation","text":"<p><code>dbworkload</code> comes already pre-packaged, available from PyPI.</p> <p>Generally, you want to specify which of the supported drivers you want to install.</p> <p>In below example, we install with the Psycopg3 driver, so we run</p> <pre><code>pip3 install dbworkload[postgres]\n</code></pre> <p>Confirm installation is successful by running</p> <pre><code>$ dbworkload --version\ndbworkload : 0.5.1\nPython     : 3.11.3\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>The best way to understand how <code>dbworkload</code> works is by running through a good example.</p> <p>We will be using PostgreSQL Server and CockroachDB, but the same logic can be applied to any of the other supported technologies.</p> <p>In this tutorial, we will go through the following tasks:</p> <ol> <li>As a prerequisite, we setup our working environment.</li> <li>We start with the DDL of a few tables and the SQL statements that are routinely executed against these tables.</li> <li>We generate some random datasets for seeding the database tables.</li> <li>We then create the tables in our database, and import the generated dataset.</li> <li>From the SQL statements, we create the <code>dbworload</code> class file, that is, our workload file.</li> <li>We run the workload, saving stats to a CSV file.</li> <li>We collect the information and plot a chart to display the results of the test run.</li> </ol>"},{"location":"getting_started/0/","title":"Preparation","text":"<p>For this tutorial, we assume you have a CockroachDB or PostgreSQL Server running locally and accessible at <code>localhost</code>.</p> <p>Make sure <code>dbworkload</code> is installed on your machine, see how at Installation.</p> <p>Create a directory <code>workloads</code> to store all our files.</p> <pre><code>mkdir workloads\ncd workloads\n</code></pre> <p>For reference, here are the URI used to connect to my local instances</p>"},{"location":"getting_started/0/#postgresql","title":"PostgreSQL","text":"<pre><code>$ psql 'postgres://fabio:postgres@localhost:5432/postgres?sslmode=disable'\nTiming is on.\npsql (16.2)\nType \"help\" for help.\n\npostgres=# select version();\n                                                              version                                                               \n------------------------------------------------------------------------------------------------------------------------------------\n PostgreSQL 16.2 (Postgres.app) on aarch64-apple-darwin21.6.0, compiled by Apple clang version 14.0.0 (clang-1400.0.29.102), 64-bit\n(1 row)\n\nTime: 1.926 ms\npostgres=# \n</code></pre>"},{"location":"getting_started/0/#cockroachdb","title":"CockroachDB","text":"<pre><code>$ cockroach sql --url 'postgres://cockroach:cockroach@localhost:26257/defaultdb?sslmode=require'\n#\n# Welcome to the CockroachDB SQL shell.\n# All statements must be terminated by a semicolon.\n# To exit, type: \\q.\n#\n# Server version: CockroachDB CCL v24.2.3 (x86_64-apple-darwin19, built 2024/09/23 22:30:57, go1.22.5 X:nocoverageredesign) (same version as client)\n# Cluster ID: e360faa9-2ba3-4e92-bd51-fc7e88cf24a8\n# Organization: Workshop\n#\n# Enter \\? for a brief introduction.\n#\ncockroach@localhost:26257/defaultdb&gt; select version();\n                                                   version\n-------------------------------------------------------------------------------------------------------------\n  CockroachDB CCL v24.2.3 (x86_64-apple-darwin19, built 2024/09/23 22:30:57, go1.22.5 X:nocoverageredesign)\n(1 row)\n\nTime: 1ms total (execution 1ms / network 0ms)\n\ncockroach@localhost:26257/defaultdb&gt;\n</code></pre>"},{"location":"getting_started/1/","title":"Schema and SQL statements","text":"<p>Our starting point is the schema and the SQL statements we want to eventually emulate.</p>"},{"location":"getting_started/1/#ddl","title":"DDL","text":"<p>For this tutorial, the DDL is very simple, just 2 tables.</p> <pre><code>-- file: bank.ddl\n\nCREATE TABLE ref_data (\n    acc_no BIGINT PRIMARY KEY,\n    external_ref_id UUID,\n    created_time TIMESTAMPTZ,\n    acc_details VARCHAR\n);\n\nCREATE TABLE orders (\n    acc_no BIGINT NOT NULL,\n    id UUID NOT NULL default gen_random_uuid(),\n    status VARCHAR NOT NULL,\n    amount DECIMAL(15, 2),\n    ts TIMESTAMPTZ default now(),\n    CONSTRAINT pk PRIMARY KEY (acc_no, id)\n);\n</code></pre> <p>Note</p> <p>PostgreSQL and CockroachDB share the same SQL syntax as CockroachDB has adopted the postgres wire API protocol. This means the above <code>CREATE TABLE</code> statements work for both technologies \ud83d\ude80</p> <p>Save the DDL to a file <code>bank.ddl</code>.</p>"},{"location":"getting_started/1/#statements","title":"Statements","text":"<p>This below is the list of statements that constitute our workload. Bind parameters, denoted with <code>%s</code>, will vary, and we are expected to simulate them.</p> <pre><code>-- file: bank.sql\n\n-- read operation, executed 50% of the time\nSELECT * FROM orders WHERE acc_no = %s AND id = %s;\n\n-- below 2 transactions constitute a complete order execution\n\n-- new_order\nINSERT INTO orders (acc_no, status, amount) VALUES (%s, 'Pending', %s) RETURNING id;\n\n-- execute order - this is an explicit transaction\nSELECT * FROM ref_data WHERE acc_no = %s;\nUPDATE orders SET status = 'Complete' WHERE (acc_no, id) = (%s, %s);\n</code></pre> <p>Save this to file <code>bank.sql</code></p> <p>A quick note</p> <p>For this exercise, we will not spend too much time caring about what the statements do. The statements in this example will not make much sense, so just focus on <code>dbworkload</code> \ud83d\ude42</p>"},{"location":"getting_started/2/","title":"Seed the database tables","text":""},{"location":"getting_started/2/#create-the-tables","title":"Create the tables","text":"<p>Create the tables in a database called <code>bank</code>.</p>"},{"location":"getting_started/2/#postgresql","title":"PostgreSQL","text":"<pre><code>postgres=# CREATE DATABASE bank;\n</code></pre> <p>Now, you need to disconnect and reconnect to <code>bank</code></p> <pre><code>psql 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable'\n</code></pre> <p>Once on the SQL prompt, import the file, or copy-paste, as you prefer.</p> <pre><code>bank=# \\i bank.sql\n\nbank=# \\d\n--          List of relations\n--  Schema |   Name   | Type  | Owner \n-- --------+----------+-------+-------\n--  public | orders   | table | fabio\n--  public | ref_data | table | fabio\n-- (2 rows)\n</code></pre>"},{"location":"getting_started/2/#cockroachdb","title":"CockroachDB","text":"<pre><code>defaultdb&gt; CREATE DATABASE bank;\n\ndefaultdb&gt; USE bank;\n\nbank&gt; \\i bank.sql\n\nbank&gt; SHOW TABLES;\n--   schema_name | table_name | type  |   owner   | estimated_row_count | locality\n-- --------------+------------+-------+-----------+---------------------+-----------\n--   public      | orders     | table | cockroach |                   0 | NULL\n--   public      | ref_data   | table | cockroach |                   0 | NULL\n-- (2 rows)\n</code></pre>"},{"location":"getting_started/2/#generate-datasets","title":"Generate datasets","text":"<p>Next, generate some CSV data to seed the database. <code>dbworkload</code> has 2 built-in utility functions that can assist with this task:</p> <ul> <li><code>dbworkload util yaml</code>, which converts a DDL file into a data generation definition file, structured in YAML.</li> <li><code>dbworkload util csv</code>, which takes the YAML file as input and outputs CSV files.</li> </ul> <p>Read the docs for util commands <code>yaml</code> and <code>csv</code> for more information.</p> <p>Let's use the <code>yaml</code> utility with our <code>bank.ddl</code> file.</p> <pre><code>dbworkload util yaml -i bank.ddl\n</code></pre> <p>For this exercise, we will use below simplified YAML file. Replace the content of <code>bank.yaml</code> with below YAML</p> <pre><code>ref_data:\n- count: 1000\n  sort-by: \n    - acc_no\n  columns:\n    acc_no:\n      type: sequence\n      args:\n        start: 0\n    external_ref_id:\n      type: UUIDv4\n      args: {}\n    created_time:\n      type: timestamp\n      args:\n        start: '2000-01-01'\n        end: '2024-12-31'\n        format: '%Y-%m-%d %H:%M:%S.%f'\n    acc_details:\n      type: string\n      args: \n        min: 10\n        max: 30\n</code></pre> <p>Now let's create a CSV dataset</p> <pre><code>dbworkload util csv -i bank.yaml -x 1\n</code></pre> <p>The CSV files will be located inside a <code>bank</code> directory.</p> <pre><code>$ ls -lh1 bank\nref_data.0_0_0.tsv\n\n$ wc -l bank/*\n    1000 bank/ref_data.0_0_0.tsv\n</code></pre> <p>Inspect it</p> <pre><code>$ head -n5 bank/ref_data.0_0_0.tsv \n0       3a2edc9d-a96b-4541-99ae-0098527545f7    2008-03-19 06:20:27.209214      CWUh0FWashpmWCx4LF3kb1\n1       829de6d6-103c-4707-9668-c4359ef5373c    2014-02-13 22:04:20.168239      QGspICZBHYpRLnHNcg\n2       5dd183af-d728-4e12-8b11-2900b6f6880a    2019-04-01 16:14:40.388236      sEUukccOePdnIbiQyVUSi0HS7rL\n3       21f00778-5fca-4302-8380-56fa461adfc8    2003-05-21 19:21:21.598455      OQTNwxoZIAdNmcA6fJM5eGDvMJgKJ\n4       035dac61-b4a3-40a4-9e4d-0deb50fef3ae    2011-08-15 06:15:40.405698      RvToVnn20BEXoxFzw9QFpCt\n</code></pre>"},{"location":"getting_started/2/#importing-datasets","title":"Importing datasets","text":"<p>Now we are ready to import the CSV file into our table <code>ref_data</code>.</p>"},{"location":"getting_started/2/#postgresql_1","title":"PostgreSQL","text":"<p>For PostgreSQL Server, at the SQL prompt, just use <code>COPY</code></p> <pre><code>bank=# COPY ref_data FROM '/path/to/workloads/bank/ref_data.0_0_0.csv' WITH CSV DELIMITER AS e'\\t';\nCOPY 1000\nTime: 8.713 ms\n</code></pre>"},{"location":"getting_started/2/#cockroachdb_1","title":"CockroachDB","text":"<p>For CockroachDB, my favorite method is to use a webserver for serving the CSV files.</p> <p>Open a new terminal then start a simple python server</p> <pre><code>cd workloads/bank\npython3 -m http.server 3000\n</code></pre> <p>If you open your browser at http://localhost:3000, you should see file <code>ref_data.0_0_0.tsv</code> being served.</p> <p>At the SQL prompt, import the file</p> <pre><code>bank&gt; IMPORT INTO ref_data CSV DATA ('http://localhost:3000/ref_data.0_0_0.tsv') WITH delimiter = e'\\t', nullif = '';\n        job_id        |  status   | fraction_completed | rows | index_entries | bytes\n----------------------+-----------+--------------------+------+---------------+--------\n  1013454367369822209 | succeeded |                  1 | 1000 |             0 | 71401\n(1 row)\n</code></pre>"},{"location":"getting_started/3/","title":"Create the workload class","text":"<p>We now need to create the Python class file that describes our workload.</p> <p>This is going to be a regular <code>.py</code> file which needs to be created from scratch.</p> <p>Fortunately, we can use built-in function <code>dbworkload util gen_stub</code> to generate a skeleton which we can use as a base to get started.</p> <p>Read the helpful doc page for the gen_stub command.</p>"},{"location":"getting_started/3/#create-the-stub-file","title":"Create the stub file","text":"<p>Execute the following command</p> <pre><code>dbworkload util gen_stub -i bank.sql\n</code></pre> <p>A new file, <code>bank.py</code>, will be created in your directory.</p>"},{"location":"getting_started/3/#review-the-workload-class-file","title":"Review the workload class file","text":"<p>The file is pretty long to be pasted here all in one shot, so we will go through it in sections</p>"},{"location":"getting_started/3/#library-imports","title":"Library imports","text":"<p>The first few lines define the imports</p> <pre><code>import datetime as dt\nimport psycopg\nimport random\nimport time\nfrom uuid import uuid4\n</code></pre> <p>These are the most common libraries used when using a workload class. More can be added as needed.</p>"},{"location":"getting_started/3/#class-init","title":"Class init","text":"<p>Here is the definition of class <code>Bank</code>.</p> <pre><code>class Bank:\n    def __init__(self, args: dict):\n        # args is a dict of string passed with the --args flag\n\n        self.think_time: float = float(args.get(\"think_time\", 5) / 1000)\n\n        # you can arbitrarely add any variables you want\n        self.my_var = 1\n\n        # translation table for efficiently generating a string\n        # -------------------------------------------------------\n        # make translation table from 0..255 to A..Z, 0..9, a..z\n        # the length must be 256\n        self.tbl = bytes.maketrans(\n            bytearray(range(256)),\n            bytearray(\n                [ord(b\"a\") + b % 26 for b in range(113)]\n                + [ord(b\"0\") + b % 10 for b in range(30)]\n                + [ord(b\"A\") + b % 26 for b in range(113)]\n            ),\n        )\n</code></pre> <p>The <code>init</code> adds some convenient examples on how to pass runtime arguments and set class variables. <code>self.tbl</code> is used to generate random string - we will review that in the next sections.</p>"},{"location":"getting_started/3/#setup","title":"setup()","text":"<p>Next up is <code>setup()</code>. <code>dbworkload</code> invokes this function when it first starts up.</p> <pre><code>    # the setup() function is executed only once\n    # when a new executing thread is started.\n    # Also, the function is a vector to receive the excuting threads's unique id and the total thread count\n    def setup(self, conn: psycopg.Connection, id: int, total_thread_count: int):\n        with conn.cursor() as cur:\n            print(\n                f\"My thread ID is {id}. The total count of threads is {total_thread_count}\"\n            )\n            print(cur.execute(f\"select version()\").fetchone()[0])\n</code></pre>"},{"location":"getting_started/3/#loop","title":"loop()","text":"<p>Function <code>loop()</code> is what is repeatedly executed by <code>dbworkload</code>. In the return list you define the function/transactions you want to execute.</p> <pre><code>    # the loop() function returns a list of functions\n    # that dbworkload will execute, sequentially.\n    # Once every func has been executed, loop() is re-evaluated.\n    # This process continues until dbworkload exits.\n    def loop(self):\n        return [\n            self.txn_0,\n            self.txn_1,\n            self.txn_2,\n            self.txn_3,\n        ]\n</code></pre>"},{"location":"getting_started/3/#utility-functions","title":"Utility functions","text":"<p>Here the stub includes some commonly used functions, such as <code>random_str</code> to generate just that.</p> <pre><code>    #####################\n    # Utility Functions #\n    #####################\n    def __think__(self, conn: psycopg.Connection):\n        time.sleep(self.think_time)\n\n    def random_str(self, size: int = 12):\n        return (\n            random.getrandbits(8 * size)\n            .to_bytes(size, \"big\")\n            .translate(self.tbl)\n            .decode()\n        )\n</code></pre>"},{"location":"getting_started/3/#workload-transactions","title":"Workload transactions","text":"<p>Next are the stub of the functions generated from the SQL statements in the <code>bank.sql</code> file.</p> <p>This is the read operation</p> <pre><code>    def txn_0(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT *\n                FROM orders\n                WHERE acc_no = %s\n                  AND id = %s\n                \"\"\",\n                (\n                    # add bind parameter,\n                    # add bind parameter, \n                ), \n            ).fetchall()\n</code></pre> <p>And below are the transactions related to the order</p> <pre><code>    def txn_1(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                INSERT INTO orders (acc_no, status, amount)\n                VALUES (%s, 'Pending', %s) RETURNING id\n                \"\"\",\n                (\n                    # add bind parameter,\n                    # add bind parameter, \n                ), \n            ).fetchall()\n\n    def txn_2(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT *\n                FROM ref_data\n                WHERE acc_no = %s\n                \"\"\",\n                (\n                    # add bind parameter, \n                ), \n            ).fetchall()\n\n    def txn_3(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                UPDATE orders\n                SET status = 'Complete'\n                WHERE (acc_no,\n                       id) = (%s,\n                              %s)\n                \"\"\",\n                (\n                    # add bind parameter,\n                    # add bind parameter, \n                ), \n            )\n</code></pre>"},{"location":"getting_started/3/#helpful-tips","title":"Helpful tips","text":"<p>Finally, the stub finishes with some tips on how to generate random data</p> <pre><code>'''\n# Quick random generators reminder\n\n# random string of 25 chars\nself.random_str(25),\n\n# random int between 0 and 100k\nrandom.randint(0, 100000),\n\n# random float with 2 decimals \nround(random.random()*1000000, 2)\n\n# now()\ndt.datetime.utcnow()\n\n# random timestamptz between certain dates,\n# expressed as unix ts\ndt.datetime.fromtimestamp(random.randint(1655032268, 1759232268))\n\n# random UUID\nuuid4()\n\n# random bytes\nsize = 12\nrandom.getrandbits(8 * size).to_bytes(size, \"big\")\n\n'''\n</code></pre> <p>In the next session, we customize the file to fit our workload.</p> <p>Psycopg 3 basic usage</p> <p>It might be a good idea now to refresh how the <code>psycopg</code> driver works. Fortunately, there is a great intro doc in the official website \ud83d\ude80</p>"},{"location":"getting_started/4/","title":"Edit the workload","text":"<p>Ok, we have only a few changes to make to our stub file so that it matches our intended workload.</p>"},{"location":"getting_started/4/#read-transaction","title":"Read Transaction","text":"<p>The SELECT statement requires us to pass <code>acc_no</code> and <code>id</code>.</p> <p><code>acc_no</code> is a number between 0 and 999. This matches the data we generated for table <code>ref_data</code>.</p> <p><code>id</code> is a UUID and it is supposed to be generated by the database upon insertion.</p> <p>So how can we pass a valid <code>(acc_no, id)</code> to our query? We can keep track of, say, 10,000 values (tuples) into a list, and pick from it randomly. The list will be populated as we insert new orders into the database, so after a few 1000's iterations, our list will be full and the SELECT will pass existing values.</p> <p>A convenient Python object that can help is the deque, which allows us to define a max length.</p> <p>We also need a way to control how many read transactions we want to do compared to order transactions. For this, we pass an argument at runtime.</p>"},{"location":"getting_started/4/#add-required-library","title":"Add required library","text":"<pre><code>import datetime as dt\nimport psycopg\nimport random\nimport time\nfrom uuid import uuid4\nfrom collections import deque\n</code></pre>"},{"location":"getting_started/4/#add-variables-and-runtime-argument","title":"Add variables and runtime argument","text":"<p>We add here the runtime argument <code>read_pct</code> and 2 variables:</p> <ul> <li>the deque object called <code>order_tuples</code></li> <li>the <code>account_number</code> and <code>id</code> needed for the Order transaction.</li> </ul> <pre><code>class Bank:\n    def __init__(self, args: dict):\n        # args is a dict of string passed with the --args flag\n\n        self.think_time: float = float(args.get(\"think_time\", 5) / 1000)\n\n        # Runtime argument to control read vs order txns\n        # Percentage of read operations compared to order operations\n        self.read_pct: float = float(args.get(\"read_pct\", 50) / 100)\n\n        # initiate deque with a random tuple so a read won't fail\n        self.order_tuples = deque([(0, uuid4())], maxlen=10000)\n\n        # keep track of the current account number and id\n        self.account_number = 0\n        self.id = uuid4()\n\n        # translation table for efficiently generating a string\n        # -------------------------------------------------------\n        # make translation table from 0..255 to A..Z, 0..9, a..z\n        # the length must be 256\n        self.tbl = bytes.maketrans(\n            bytearray(range(256)),\n            bytearray(\n                [ord(b\"a\") + b % 26 for b in range(113)]\n                + [ord(b\"0\") + b % 10 for b in range(30)]\n                + [ord(b\"A\") + b % 26 for b in range(113)]\n            ),\n        )\n</code></pre>"},{"location":"getting_started/4/#add-bind-parameters-to-read-function","title":"Add bind parameters to read function","text":"<p>We rename the function to something more descriptive</p> <pre><code>    def txn_read(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT *\n                FROM orders\n                WHERE acc_no = %s\n                  AND id = %s\n                \"\"\",\n                random.choice(self.order_tuples),  \n            ).fetchall()\n</code></pre> <p>That's it! Our read operation is complete \ud83d\ude80.</p>"},{"location":"getting_started/4/#order-transaction","title":"Order transaction","text":"<p>The order transaction is made up of 2 distinct database transactions:</p> <ul> <li>new_order</li> <li>order_exec</li> </ul>"},{"location":"getting_started/4/#new-order","title":"New Order","text":"<p>This is the first of the 2 transactions of our Order workload.</p> <pre><code>    def txn_new_order(self, conn: psycopg.Connection):\n\n        # generate a random account number to be used for\n        # for the order transaction\n        self.account_number = random.randint(0, 999)\n\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                INSERT INTO orders (acc_no, status, amount)\n                VALUES (%s, 'Pending', %s) RETURNING id\n                \"\"\",\n                (\n                    self.account_number,\n                    round(random.random() * 1000000, 2),\n                ),\n            )\n\n            # save the id that the server generated\n            self.id = cur.fetchone()[0]\n\n            # save the (acc_no, id) tuple to our deque list\n            # for future read transactions\n            self.order_tuples.append((self.account_number, self.id))\n</code></pre>"},{"location":"getting_started/4/#order-execution","title":"Order Execution","text":"<p>The order execution transaction is an explicit transaction.</p> <pre><code>    def txn_order_exec(self, conn: psycopg.Connection):\n\n        # with Psycopg, this is how you start an explicit transaction\n        with conn.transaction() as tx:\n            with conn.cursor() as cur:\n                cur.execute(\n                    \"\"\"\n                    SELECT *\n                    FROM ref_data\n                    WHERE acc_no = %s\n                    \"\"\",\n                    (\n                        self.account_number,\n                    ),\n                ).fetchall()\n\n                # simulate microservice doing something...\n                time.sleep(0.02)\n\n                cur.execute(\n                    \"\"\"\n                    UPDATE orders\n                    SET status = 'Complete'\n                    WHERE \n                        (acc_no, id) = (%s, %s)\n                    \"\"\",\n                    (\n                        self.account_number,\n                        self.id,\n                    ),\n                )\n</code></pre>"},{"location":"getting_started/4/#update-loop","title":"Update <code>loop()</code>","text":"<p>The final change is to update the <code>loop()</code> function to return either a read txn or an order txn.</p> <pre><code>    def loop(self):\n        if random.random() &lt; self.read_pct:\n            return [self.txn_read]\n        return [self.txn_new_order, self.txn_order_exec]\n</code></pre>"},{"location":"getting_started/4/#full-class-file-bankpy","title":"Full class file <code>bank.py</code>","text":"<p>For completeness, here is the full edited file, minus the helpful tips</p> <pre><code>import datetime as dt\nimport psycopg\nimport random\nimport time\nfrom uuid import uuid4\nfrom collections import deque\n\n\nclass Bank:\n    def __init__(self, args: dict):\n        # args is a dict of string passed with the --args flag\n\n        self.think_time: float = float(args.get(\"think_time\", 5) / 1000)\n\n        # Percentage of read operations compared to order operations\n        self.read_pct: float = float(args.get(\"read_pct\", 0) / 100)\n\n        # initiate deque with 1 random UUID so a read won't fail\n        self.order_tuples = deque([(0, uuid4())], maxlen=10000)\n\n        # keep track of the current account number and id\n        self.account_number = 0\n        self.id = uuid4()\n\n        # translation table for efficiently generating a string\n        # -------------------------------------------------------\n        # make translation table from 0..255 to A..Z, 0..9, a..z\n        # the length must be 256\n        self.tbl = bytes.maketrans(\n            bytearray(range(256)),\n            bytearray(\n                [ord(b\"a\") + b % 26 for b in range(113)]\n                + [ord(b\"0\") + b % 10 for b in range(30)]\n                + [ord(b\"A\") + b % 26 for b in range(113)]\n            ),\n        )\n\n    # the setup() function is executed only once\n    # when a new executing thread is started.\n    # Also, the function is a vector to receive the excuting threads's unique id and the total thread count\n    def setup(self, conn: psycopg.Connection, id: int, total_thread_count: int):\n        with conn.cursor() as cur:\n            print(\n                f\"My thread ID is {id}. The total count of threads is {total_thread_count}\"\n            )\n            print(cur.execute(f\"select version()\").fetchone()[0])\n\n    # the loop() function returns a list of functions\n    # that dbworkload will execute, sequentially.\n    # Once every func has been executed, loop() is re-evaluated.\n    # This process continues until dbworkload exits.\n    def loop(self):\n        if random.random() &lt; self.read_pct:\n            return [self.txn_read]\n        return [self.txn_new_order, self.txn_order_exec]\n\n    #####################\n    # Utility Functions #\n    #####################\n    def __think__(self, conn: psycopg.Connection):\n        time.sleep(self.think_time)\n\n    def random_str(self, size: int = 12):\n        return (\n            random.getrandbits(8 * size)\n            .to_bytes(size, \"big\")\n            .translate(self.tbl)\n            .decode()\n        )\n\n    # Workload function stubs\n\n    def txn_read(self, conn: psycopg.Connection):\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT *\n                FROM orders\n                WHERE acc_no = %s\n                  AND id = %s\n                \"\"\",\n                random.choice(self.order_tuples),\n            ).fetchall()\n\n    def txn_new_order(self, conn: psycopg.Connection):\n\n        # generate a random account number to be used for\n        # for the order transaction\n        self.account_number = random.randint(0, 999)\n\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                INSERT INTO orders (acc_no, status, amount)\n                VALUES (%s, 'Pending', %s) RETURNING id\n                \"\"\",\n                (\n                    self.account_number,\n                    round(random.random() * 1000000, 2),\n                ),\n            )\n\n            # save the id that the server generated\n            self.id = cur.fetchone()[0]\n\n            # save the (acc_no, id) tuple to our deque list\n            # for future read transactions\n            self.order_tuples.append((self.account_number, self.id))\n\n    def txn_order_exec(self, conn: psycopg.Connection):\n\n        # with Psycopg, this is how you start an explicit transaction\n        with conn.transaction() as tx:\n            with conn.cursor() as cur:\n                cur.execute(\n                    \"\"\"\n                    SELECT *\n                    FROM ref_data\n                    WHERE acc_no = %s\n                    \"\"\",\n                    (\n                        self.account_number,\n                    ),\n                ).fetchall()\n\n                # simulate microservice doing something...\n                time.sleep(0.02)\n\n                cur.execute(\n                    \"\"\"\n                    UPDATE orders\n                    SET status = 'Complete'\n                    WHERE \n                        (acc_no, id) = (%s, %s)\n                    \"\"\",\n                    (\n                        self.account_number,\n                        self.id,\n                    ),\n                )\n</code></pre> <p>We are now finally ready to run our workload!</p>"},{"location":"getting_started/5/","title":"Test the workload class","text":"<p>As a test, we want to try to execute only 1 function at the time, once. We want to make sure we don't run into any runtime errors.</p>"},{"location":"getting_started/5/#test-the-read-transaction","title":"Test the Read transaction","text":"<pre><code># we set read_pct to 100 to force just the read_txn\n# -i stands for iterations\n\n# PostgreSQL\ndbworkload run -w bank.py --uri 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable' \\\n  -i 1 --args '{\"read_pct\": 100}'\n\n# CockroachDB - notice we set the uri to point at the `bank` database \u2b07\ufe0f\u2b07\ufe0f \ndbworkload run -w bank.py -c 4 --uri 'postgres://root@localhost:26257/bank?sslmode=disable' \\\n  -i 1 --args '{\"read_pct\": 100}'\n</code></pre> <p>The output is the same for any driver you choose, and it looks like below, stripped of some log messages for brevity.</p> <p>As in the <code>setup()</code> function we execute <code>select version();</code>, you will see in below output that this is the output from running <code>dbworkload</code> against PostgreSQL server.</p> <pre><code>2024-10-20 11:21:03,294 [INFO] (MainProcess MainThread) run:269: Starting workload Bank.20241020_152103\nMy thread ID is 0. The total count of threads is 1\nPostgreSQL 16.2 (Postgres.app) on aarch64-apple-darwin21.6.0, compiled by Apple clang version 14.0.0 (clang-1400.0.29.102), 64-bit\n2024-10-20 11:21:04,048 [INFO] (MainProcess MainThread) run:369: Requested iteration/duration limit reached\n2024-10-20 11:21:04,050 [INFO] (MainProcess MainThread) run:176: Printing final stats\n  elapsed  id           threads    tot_ops    tot_ops/s    period_ops    period_ops/s    mean(ms)    p50(ms)    p90(ms)    p95(ms)    p99(ms)    max(ms)\n---------  ---------  ---------  ---------  -----------  ------------  --------------  ----------  ---------  ---------  ---------  ---------  ---------\n        1  __cycle__          1          1            1             1               1        3.59       3.59       3.59       3.59       3.59       3.59\n        1  txn_read           1          1            1             1               1        3.58       3.58       3.58       3.58       3.58       3.58 \n\n2024-10-20 11:21:04,051 [INFO] (MainProcess MainThread) run:181: Printing summary for the full test run\n[...]\n</code></pre>"},{"location":"getting_started/5/#test-the-order-transaction","title":"Test the Order Transaction","text":"<p>We can repeat the  exercise to test the Order Transaction functions</p> <pre><code># now we set read_pct to zero\n\n# PostgreSQL\ndbworkload run -w bank.py --uri 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable' \\\n  -i 1 --args '{\"read_pct\": 0}'\n\n# CockroachDB \ndbworkload run -w bank.py -c 4 --uri 'postgres://root@localhost:26257/bank?sslmode=disable' \\\n  -i 1 --args '{\"read_pct\": 0}'\n</code></pre> <p>Here's the output from CockroachDB</p> <pre><code>2024-10-20 11:31:07,678 [INFO] (MainProcess MainThread) run:269: Starting workload Bank.20241020_153107\nMy thread ID is 0. The total count of threads is 1\nCockroachDB CCL v24.2.3 (x86_64-apple-darwin19, built 2024/09/23 22:30:57, go1.22.5 X:nocoverageredesign)\n2024-10-20 11:31:08,507 [INFO] (MainProcess MainThread) run:369: Requested iteration/duration limit reached\n2024-10-20 11:31:08,509 [INFO] (MainProcess MainThread) run:176: Printing final stats\n  elapsed  id                threads    tot_ops    tot_ops/s    period_ops    period_ops/s    mean(ms)    p50(ms)    p90(ms)    p95(ms)    p99(ms)    max(ms)\n---------  --------------  ---------  ---------  -----------  ------------  --------------  ----------  ---------  ---------  ---------  ---------  ---------\n        1  __cycle__               1          1            1             1               1       89.67      89.67      89.67      89.67      89.67      89.67\n        1  txn_new_order           1          1            1             1               1       21.20      21.20      21.20      21.20      21.20      21.20\n        1  txn_order_exec          1          1            1             1               1       68.45      68.45      68.45      68.45      68.45      68.45 \n\n2024-10-20 11:31:08,511 [INFO] (MainProcess MainThread) run:181: Printing summary for the full test run\n[...]\n</code></pre> <p>Success! The workload class runs without throwing any runtime errors \ud83d\ude80</p> <p>Important</p> <p>You should always, as a good practice, follow these 2 instructions:</p> <ul> <li>use <code>print()</code> throughout your workload class file to make sure you are generating, retrieving and querying the correct data.   Print out your bind parameters, the result of <code>cur.fetchone()</code>, your variables like <code>self.order_tuples</code>.</li> <li>inspect the mutated data on the SQL prompt matches with the intended workload goal.</li> </ul>"},{"location":"util/","title":"Utility functions","text":"<p>Here is the list of utility functions available</p> function description csv Generate CSV files from a YAML data generation file. gen_stub Generate a dbworkload class stub. html Save charts to HTML from the dbworkload statistics CSV file. merge Merge multiple sorted CSV files into 1+ files. merge_csvs Merge multiple dbworkload statistic CSV files. plot Plot terminal charts from the statistics CSV file. yaml Generate YAML data generation file from a DDL SQL file."},{"location":"util/csv/","title":"csv","text":""},{"location":"util/csv/#generating-csv-files","title":"Generating CSV files","text":"<p>You can seed a database quickly by letting <code>dbworkload</code> generate pseudo-random data and import it.</p> <p><code>dbworkload</code> takes the DDL as an input and creates an intermediate YAML file, with the definition of what data you want to create (a string, a number, a date, a bool..) based on the column data type.</p> <p>You then refine the YAML file to suit your needs, for example, the size of the string, a range for a date, the precision for a decimal, a choice among a discrete list of values..</p> <p>You can also specify what is the percentage of NULL for any column, or how many elements in an ARRAY type. You then specify the total row count, how many rows per file, and in what order, if any, to sort by.</p> <p>Then <code>dbworkload</code> will generate the data into CSV or TSV files, compress them if so requested.</p> <p>You can then optionally merge-sort the files.</p> <p>Consult file <code>workloads/postgres/bank.yaml</code> for a list of all available generators and options.</p>"},{"location":"util/csv/#see-also","title":"See also","text":"<ul> <li> <p>Generating intermediate data definition YAML file</p> </li> <li> <p>Blog: Generate multiple large sorted CSV files with pseudo-random data</p> </li> </ul>"},{"location":"util/gen_stub/","title":"gen_stub","text":""},{"location":"util/html/","title":"html","text":""},{"location":"util/merge/","title":"merge","text":""},{"location":"util/merge_csvs/","title":"merge_csvs","text":""},{"location":"util/plot/","title":"plot","text":""},{"location":"util/yaml/","title":"yaml","text":""},{"location":"util/yaml/#generate-yaml-data-generation-file-from-a-ddl-sql-file","title":"Generate YAML data generation file from a DDL SQL file","text":"<p>CREATE TABLE ref_data (     my_sequence BIGINT PRIMARY KEY,     my_constant VARCHAR,     my_uuid UUID,     my_choice VARCHAR,     my_integer BIGINT,     my_float FLOAT ARRAY,     my_decimal DECIMAL(15, 4),     my_timestamp TIMESTAMPTZ,     my_date DATE,     my_time TIME,     my_bit BIT(10),     my_bytes BYTEA,     my_string VARCHAR[],     my_bool BOOL,     my_json JSONB );</p>"}]}